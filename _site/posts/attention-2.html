
<!doctype>
<html lang="en">
  <head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <meta content='Teaching attention, part 2 / N: In which I don't even talk about attention - Info, Uncertainty' name='title' />
    <meta content='Teaching attention, part 2 / N: In which I don't even talk about attention - Info, Uncertainty' name='og:title' />
    <title>Teaching attention, part 2 / N: In which I don't even talk about attention - Info, Uncertainty</title>
    <link href='http://localhost:4000/images/fav.png' rel='shortcut icon'>
<link href='http://localhost:4000/stylesheets/style.css' rel='stylesheet' type='text/css' />
<link href='http://localhost:4000/stylesheets/syntax.css' rel='stylesheet' type='text/css' />

<meta content='width=device-width, initial-scale=1.0, user-scalable=no' name='viewport'>
<meta content='text/html; charset=utf-8' http-equiv='content-type' />

  <meta content='http://localhost:4000/posts/attention-2' property='og:url' />
  <meta content="It’s a little cliche to start a post by saying that a certain type of data (inthis case, sequential data) are everywh..." property='og:description' />
  <meta content="article" property="og:type" />

<!-- - -->





  </head>
  <body class="lh-copy dark-gray pa0 f6 sans-serif bg-super-white">
    <header class="tc mt4">
      <a href="http://localhost:4000">
        <img src="http://localhost:4000/images/distn.png" alt="Home" width="53" height="59">
      </a>
      <p>Info, Uncertainty</p>
    </header>
    <div class="mw7 bg-white mt4 mb3 center br2-ns bt bb ba-ns b--light-gray">
      <nav class="bb b--light-gray pv4 tc" aria-label="Main">
        
          <a class="link blue hover-mid-gray mh2 pv1"
             href="http://localhost:4000/">
             Posts
           </a>
        
          <a class="link blue hover-mid-gray mh2 pv1"
             href="http://localhost:4000/about">
             About
           </a>
        
      </nav>

      <main class="tl f6 relative pa4 pa5-ns overflow-hidden">
        
          <div class="mb4">
            <div class="fw600 light-silver mt1">15 Jul 2019</div>
            <h1 class="ttu f3 mt0 lh-title cb mb2">
              
              Teaching attention, part 2 / N: In which I don't even talk about attention
            </h1>
            
          </div>
        
        <div class="markdown-body">
          <p>It’s a little cliche to start a post by saying that a certain type of data (in
this case, sequential data) are everywhere. There’s a risk of being vague, and
writing things that will become outdated, like the phrase, “<a href="https://www.cia.gov/library/readingroom/docs/CIA-RDP75-00149R000100400004-6.pdf">soviet
cybernetics</a>”.</p>

<p>But sequential data <em>are</em> everywhere, and they are here to stay. The words in
this sentence are sequential data. The order of bus stops you took to get here
are sequential data. They are everywhere in society – the current capacity of
water reservoirs, the cost of rice, the reach of an influenza outbreak. They are
also everywhere in science – the trajectory of high energy particles at the
LHC, and the sequence of biological reactions that allow cells to divide. The
history of the top trending animal videos on youtube are sequential data<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>.</p>

<p>Even data that are not inherently sequential are often reformulated to be so:
large images or databases can be processed one piece at a time.</p>

<p>These data are complicated because everything is related to everything, even if
only weakly. A lot of hte videos trending from last week are still popular.</p>

<p>This interconnectedness makes things harder to manage, because you can’t just
discard a datapoint once you’ve seen it, which is more or less what we do in
settings where we claim the data are i.i.d<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup>. A the same time, trying to keep
track of everything is untenable – you would become overloaded very quickly.</p>

<p>What we need is a way to summarize and store everything that is important, as it
arrives. We also need a way of revisiting relevant information on an as-needed
basis. In the deep learning literature, these two processes are implemented
through mechanisms for memory and attention.</p>

<h2 id="rnns-revisited">RNNs, Revisited</h2>

<p>Before answering such lofty questions, we will need to have a foundation in the
basics<sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup>. We begin with the humblest of sequential data proceses, the
recurrent neural network (RNN) cell,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
h_t &= f_{\theta}\left(x_t, h_{t - 1}\right).
\end{align} %]]></script>

<p><script type="math/tex">x_t</script> are your raw input data at the <script type="math/tex">t^{th}</script> position in your sequence (“time
<script type="math/tex">t</script>”). Think of <script type="math/tex">h_{t}</script> as a running summary of everything you’ve seen so far.
Concretely, the input and summary data are just vectors: <script type="math/tex">x_t \in
\mathbb{R}^{n}, h_{t} \in \mathbb{R}^{k}</script>. The vectors are related by the
function <script type="math/tex">f_{\theta}</script>, parameterized by <script type="math/tex">\theta</script>, which updates the current
summary based on new data <script type="math/tex">x_t</script>.</p>

<p>For example, let’s say that in our summary, we only care about the number of
times our time series enters the interval <script type="math/tex">\left[0, 1\right]</script>. Our series
should look like this:</p>

<figure>
<a href="https://observablehq.com/@krisrs1128/untitled/3">
  <img src="http://localhost:4000/images/attn-2-rnn-series.png" />
</a>
  <figcaption>
  Click on the figure to go to an interactive version. The original input data
  are the upper series, and the summary is given in the lower panel, counting
  the number of distinct blue segments seen so far.
  </figcaption>
</figure>

<p>This can be easily implemented according by choosing a function <script type="math/tex">f_{\theta}</script>
appropriately<sup id="fnref:4"><a href="#fn:4" class="footnote">4</a></sup>,</p>

<script type="math/tex; mode=display">\begin{align}
h_{t + 1} = f_{\theta}\left(x_{t + 1}, h_t\right) 
:= \begin{pmatrix}
\mathbb{1}\{x_{t + 1} \in \left[0, 1\right]\} \\
h_{t,2} + \mathbb{1}\{h_{t, 1} = 0 \text{ and } x_{t + 1} \in \left[0, 1\right]\}
\end{pmatrix}
\end{align}.</script>

<p>The two coordinates of <script type="math/tex">h_t</script> encode two features. The first checks whether
<script type="math/tex">x_t</script> is in $\left[0, 1\right]$. The second coordinate checks whether we’re in
this range, and if and if at the previous step we <em>weren’t</em>, then it increments
its value, because an entrance has just occurred.</p>

<figure>
<a href="https://observablehq.com/@krisrs1128/rnn-entrance-counting">
  <img src="http://localhost:4000/images/attn-2-rnn-states.png" />
</a>
  <figcaption>
  Again, you can click on the figure to go to an interactive version. The two
  rows of rectangles correspond to the two coordinates of the hidden state. The
  first coordinate activates whenever the series passes through the blue range,
  and the second uses this to build a counter.
  </figcaption>
</figure>

<p>This is maybe one of the simplest examples of a system with memory. It takes a
complicated stream of input, and carefully records the number of <script type="math/tex">\left[0,
1\right]</script> entrances. If you task at the end of each sequence were to report
this count, then you can call it a day, this processor $f_{\theta}$ solves that
problem entirely.</p>

<h2 id="okay-its-not-that-actually-that-simple">Okay it’s not that actually that simple</h2>

<p>The general definition <script type="math/tex">f_{\theta}</script> I’ve given is a technically correct one,
but in our formulation of a counter, we’ve committed a grave sin of deep
learning: we have hard-coded our features. We looked at the problem, declared
that the number of <script type="math/tex">\left[0, 1\right]</script> entrances was what really mattered, and
hand-engineered the necessary processing unit.</p>

<p>In the real-world, this usually won’t be possible. Consider the problem of
sentiment analysis – you’re trying to tell whether a movie review is positive
or negative. No one’s going to debate that the number of occurrences of
“fantastic” and “<a href="https://www.imdb.com/list/ls076596691/">this movie is honestly the worst movie I’ve ever
seen</a>” are probably very good
predictors, but it would be foolish to try to engineer all possibly predictive
features by hand.</p>

<h2 id="enter-deep-learning">Enter, Deep Learning</h2>

<p>To resolve this, we appeal to one of the central tenets of deep learning,</p>

<blockquote>
  <p>By composing simple differentiable units, we can learn useful features
automatically.</p>
</blockquote>

<p>Graphically, we shift our perspective from chains of RNN cells,</p>

<p>where <script type="math/tex">f_{\theta}</script> might be a complex hand-engineered function, to chains <em>and
layers</em> of simpler processing units.</p>

<p>The left-to-right arrows are still used to build a type of temporal memory, but
now we have bottom-up function composition arrows, which help in learning
complex features from otherwise simple units. A common “atom” from which these
sheets are defined is the function,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
h_{t} &= \tanh\left(W_{x}x_{t} + W_{h}h_{t - 1} + b\right)
\end{align} %]]></script>

<p>which has the form <script type="math/tex">f_{\theta}\left(x_t, h_t - 1\right)</script> where <script type="math/tex">\theta =
\{W_{x}, W_{h}, b\}</script>. It’s admittedly a relatively restricted class of
functions. Through <script type="math/tex">W_{x}</script> and <script type="math/tex">W_{h}</script>, it can tell whether certain linear
mixtures of components of <script type="math/tex">x_{t}</script> or <script type="math/tex">h_{t}</script> are large or small, but in the
end it can only return monotone functions of this mixtures, and restricted to
<script type="math/tex">\left[-1, 1\right]</script> at that. That makes things sound more abstract than they
really are – you can actually visualize the entire family of functions in the
case that the input and summary are both two-dimensional,</p>

<p>These are simple, but by stacking them, we can achieve complexity, just like how
stacks of ReLU units can become complicated feature detectors in computer
vision. In fact, we can almost recover our <script type="math/tex">\left[0, 1\right]</script> entrance
counter just by stacking a few of these units.</p>

<p>Define the first layer by,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
h^{1}_{t} &= \tanh\left(\begin{pmatrix} + \\ - \end{pmatrix}x + \begin{pmatrix} 0 \\ -\end{pmatrix}\right)
\end{align} %]]></script>

<p>so that the first coordinate tells us if we’re larger than 0, and the second
tells us if we’re less than 1. Here, <script type="math/tex">+</script> and <script type="math/tex">-</script> are arbitrary positive and
negative values, respectively.</p>

<p>In the second layer, we combine these two pieces of information, to see if we’re
in the <script type="math/tex">\left[0, 1\right]</script> range,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
h_{t}^{2} &= \tanh(1^{T} h_{t}^{1}).
\end{align} %]]></script>

<p>These two units are visualized in the interactive figure linked below.</p>

<figure>
<a href="https://observablehq.com/@krisrs1128/composition-and-counting">
  <img src="http://localhost:4000/images/counter_layers.png" />
</a>
  <figcaption>
  When composed, simple, smooth monotone functions can start approximating
  things like indicators (and much more).
  </figcaption>
</figure>

<p>In the last, we need to introduce a sense of history,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
h_{t}^{3} &= \tanh\left(w_{1} h_{t - 1}^{3} + w_{2} h_{t}^{2}\right)
\end{align} %]]></script>

<p>Remember, we want to build a counter, but only have access to <script type="math/tex">\tanh</script>-like
units, which are bounded<sup id="fnref:5"><a href="#fn:5" class="footnote">5</a></sup>. The here is to use the first term, <script type="math/tex">w_{1}{h_{t -
1}^{3}}</script> to maintain a memory of the current counter state, while the second,
<script type="math/tex">w_{2}{h_{t}^{2}}</script> adds to the counter whenever it detects that we have made
a <script type="math/tex">\left[0, 1\right]</script> entrance.</p>

<p>The figure below shows that this more or less works, but that there is a
tradeoff. Making <script type="math/tex">w_{1} \gg w_{2}</script> means that increments to the count are hard
to notice, but they are retained in memory for a long time. On the other hand,
when <script type="math/tex">% <![CDATA[
w_1 << w_2 %]]></script>, we can easily see when we’ve entered the state, but there
is a risk that the counter “forgets” what it’s earlier value was.</p>

<h2 id="gating-and-memory">Gating and Memory</h2>

<p>What we’re noticing is an instance of the “vanishing gradient” problem, which is
elegantly illustrated by Figure 4.1<sup id="fnref:6"><a href="#fn:6" class="footnote">6</a></sup> from Graves’ Sequence Labeling book,</p>

<figure>
  <img src="http://localhost:4000/images/graves_vg_411.png" />
  <figcaption>
  By nature of the RNN update mechanism, new inputs gradually overwrite new
  ones, and it becomes hard for the network to remember anything from long ago.
  </figcaption>
</figure>

<p>There is a remarkably simple solution to this problem: gating. The idea is that
if the value of a feature at a particular point is potentially important for
something later on, we block off any updates to it that would otherwise be made
by all the incoming data. This process of blocking off updates is called
“gating”. It’s like we decide the current value is important, and we place it
under a glass case, where it is untouched by anything else in the world. At any
point, it’s value can be read, but not changed.</p>

<p>If at some future point, we decide we need to change the feature’s value, we
take it out of the case. We ungate what had been a gated value.</p>

<p>How do you actually implement something like this? In principle, you could
define a binary value for each feature coordinate, demarcating whether the value
can be changed at the current timestep. Then, you could try to learn a good
pattern of 0s and 1s from training examples – you might realize that certain
types of input are useful in the long run, and should always be gated.</p>

<p>In practice, learning binary patterns is difficult, since such a mask wouldn’t
be differentiable. Instead, we use sigmoid units, which are smooth surrogates
that achieve the same effect. If you pursue this line of thinking for long
enough, you woudl probably arrive at something similar (if not identical) to
LSTM or GRUs<sup id="fnref:7"><a href="#fn:7" class="footnote">7</a></sup>.</p>

<h3 id="mechanics-of-grus">Mechanics of GRUs</h3>

<p>A GRU is defined by the update,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
h_t &= \left(1 - z_t\right)h_{t - 1} + z_{t}\tilde{h}_{t}
\end{align} %]]></script>

<p>which interpolates between the previous state and some new candidate, according
to some factor <script type="math/tex">z_{t}</script>. To make <script type="math/tex">z_{t}</script> learnable, it’s set to a sigmoid
over inputs and states,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
z_t &= \sigma\left(W_z x_t + U_z h_{t - 1}\right)
\end{align} %]]></script>

<p>The candidate is tricker, because it has a notion of a reset <script type="math/tex">r_{t}</script>,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\tilde{h}_{t} &= \tanh\left(Wx_t + U\left(r_t \circ h_{t - 1}\right)\right)
\end{align} %]]></script>

<p>I think of <script type="math/tex">z_t</script> as a kind of hesitant forgetting: overwriting you have to do
in order to write anything new to <script type="math/tex">h_t</script>, while <script type="math/tex">r_t</script> is a brutal
“deliberate” forgetting, which wasn’t strictly necessary.</p>

<p>Note that to adapt the toy counter, it’s only necessary to introduce gatings
through <script type="math/tex">z_t</script>’s.</p>

<h2 id="finally-a-real-example">Finally, a real example</h2>

<p>To be honest, I didn’t want to spend so much time on gated RNNs. However,</p>

<ol>
  <li>So many of the diagrams explaining these ideas are so complicated (at least
to me, I mean, as useful as they are, does any get geometric intuition from
computational graphs?), that I couldn’t just copy and paste<sup id="fnref:8"><a href="#fn:8" class="footnote">8</a></sup>.</li>
  <li>Gating is central to understanding memory and attention. All the other
mechanisms discussed in this series are variants of the basic recipe of
knowing what to write as features for long-term reference and having clear
ways to access them when needed.</li>
</ol>

<p>I’m also a little tired of coming up with all these weights by hand – don’t
take your optimizers for granted!</p>

<blockquote>
  <p>Exercise: Train a gated RNN to perform the <script type="math/tex">\left[0, 1\right]</script> entrance
counting task, starting from a random initialization. How does the number of
layers / number of units in each layer affect the ease of training? How does
that relate to what we know about being able to hand-craft an RNN that solves
the task using only very few units.</p>
</blockquote>

<p>To see this working on a real-world task, I’ve trained a gated RNN on the
language modeling problem from this practical. Everything is the same as before,
but the <script type="math/tex">x_t</script> are now words, which are one-hot vectors in <script type="math/tex">\mathbb{R}^{V}</script>,
and the features <script type="math/tex">h_t</script> are passed into linear classifiers to predict the word
<script type="math/tex">x_{t + 1}</script>.</p>

<p>Rows of the <script type="math/tex">h_t</script> that are constant over regions are examples of memory in
action.</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>For that matter, the videos themselves are sequences of image frames. <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p><a href="https://stats.stackexchange.com/questions/17391/what-are-i-i-d-random-variables">Independent and identically distributed</a>, sometimes called “the big lie of machine learning.” <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>It turns out that the state-of-the-art are just clever combinations of these core components, anyways. <a href="#fnref:3" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>Notice that this definition doesn’t have any parameters, the notation <script type="math/tex">\theta</script> is actually superfluous here. <a href="#fnref:4" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>In theory, we could use a linear layer to get integer counts. But this would violate the principle of creating generic architectures, which discover features with minimal hand-crafted input. <a href="#fnref:5" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p>Some of the most compelling graphics are the least intricate! <a href="#fnref:6" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:7">
      <p>These are cool-kids shorthand for the otherwise dissonant phrases “Long Term Short Term Memory” and “Gated Recurrent Units.” Don’t worry about the names, we’ll explain the math in a second, at least for GRUs. <a href="#fnref:7" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:8">
      <p>Believe me, if I could, I would. <a href="#fnref:8" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

        </div>
        <p class="mt4">
  Kris
  <span class="silver">at 13:43</span>
</p>

      </main>
      <section class="fixed-l mw7 center w-100 top-50 tc pb4 nt4">
  
    <a href="http://localhost:4000/posts/attention-1" class="no-underline f1 light-blue hover-silver nl5 fl-l ph3">‹</a>
  
  
    <a href="http://localhost:4000/posts/attention-3" class="no-underline f1 light-blue hover-silver nr5 fr-l ph3">›</a>
  
</section>
    </div>
    <footer class="mw7 center tc pt3 pb4 silver">
      Built with Jekyll using <a href="http://github.com/muan/scribble" class="link silver hover-blue pv1">Scribble</a>.
      <img src="http://localhost:4000/images/scribble2.png" alt="scribble" class="mt4 db center" />
    </footer>
  </body>
</html>
