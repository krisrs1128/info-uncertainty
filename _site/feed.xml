<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Info, Uncertainty</title>
    <description></description>
    <link>http://localhost:4000</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Meditations on Mediation</title>
        <description>&lt;p&gt;Lately, I’ve been reflecting on how software design could inform statistical methods development. There has always been a two-way flow between methods and applications, and software lies right at the junction. My main thought is that an abstraction originally designed to simplify a computational interface can also guide creative statistical investigation.&lt;/p&gt;

&lt;p&gt;I think this is well-illustrated by analyzing the design of the &lt;a href=&quot;https://imai.fas.harvard.edu/research/files/mediationR2.pdf&quot;&gt;mediation package&lt;/a&gt;, so that
will be the focus of this post. In the process, we’ll review the basics of
mediation analysis and see an illustration using a microbiome-metabolomics
dataset.&lt;/p&gt;

&lt;h2 id=&quot;mediation-analysis&quot;&gt;Mediation Analysis&lt;/h2&gt;

&lt;p&gt;Mediation analysis is a form of attribution analysis. When we see that a treatment \(T\) influences an outcome \(Y\), we may want to argue that \(T\) influences \(Y\) &lt;em&gt;through a mediator&lt;/em&gt; \(M\), represented graphically by:&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
    &lt;img src=&quot;http://localhost:4000/images/mediation_software/mediation-dag.svg&quot; width=&quot;200&quot; /&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Here is an alternative visualization. The two colors represent the treatment and the control. \(Y\) depends on both the mediator and the treatment status, represented by the two curves.&lt;/p&gt;

&lt;figure&gt;
&lt;center&gt;
    &lt;img src=&quot;http://localhost:4000/images/mediation_software/counterfactual-mediation4.svg&quot; width=&quot;400&quot; /&gt;
&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;If most of the treatment effect can be attributed to the mediator, then it’s a change in the location of the mediators under treatment vs. control that is responsible for change in \(Y\) (left figure).
On the other hand, if, for a fixed value of the mediator, we recognize large changes in the response, then the effect of the treatment on the response cannot be attributed to the mediator alone (right figure).&lt;/p&gt;

&lt;figure&gt;
&lt;div class=&quot;row&quot; style=&quot;display: flex&quot;&gt;
  &lt;div class=&quot;column&quot;&gt;
    &lt;img src=&quot;http://localhost:4000/images/mediation_software/counterfactual-mediation3.svg&quot; width=&quot;300&quot; /&gt;
  &lt;/div&gt;
  &lt;div class=&quot;column&quot;&gt;
    &lt;img src=&quot;http://localhost:4000/images/mediation_software/counterfactual-mediation2.svg&quot; width=&quot;300&quot; /&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/figure&gt;

&lt;p&gt;Let’s consider a microbiome example. Intestinal Bowel Disease (IBD) is a condition that can disrupt the lives of its sufferers, and researchers have looked to the microbiome for potential therapies. To better understand how exactly the gut microbial environment differs within IBD patients, the study [&lt;a href=&quot;https://www.nature.com/articles/s41564-018-0306-4&quot;&gt;Franzosa 2019&lt;/a&gt;] gathered both metabolomic (chemical environment) and metagenomic (species abundance) profiles. One thought is that the presence or absence of particular species may mediate changes in the chemical environment for IBD patients. For example, if species \(m\) produces metabolite \(y\), then if we see a decrease in \(y\) among IBD patients, that might be the consequence of species \(m\) being absent. In the notation above, \(T\) is IBD status, the metabolite concentration \(Y\) is the outcome, and the species abundance \(M\) is the mediator.&lt;/p&gt;

&lt;p&gt;The most straightforward implementation of mediation analysis assumes the data were drawn from chained linear regressions:&lt;/p&gt;

\[\begin{align}
m_{i} &amp;amp;= \alpha t_{i} + \epsilon_{i}^{m} \\
y_{i} &amp;amp;= \beta_{t} t_{i} + \beta_{m} m_{i} + \epsilon_{i}^{y}
\end{align}\]

&lt;p&gt;In this notation, the indirect effect that flows through the mediator is computed as \(\alpha \beta_{m}\), while the direct effect which bypasses the mediator is given by \(\beta_{t}\). If we wanted to consider several metabolites and species simultaneously, then we could consider high-dimensional \(M\) and \(Y\) and vector-valued coefficients.&lt;/p&gt;

&lt;h2 id=&quot;general-mediation-analysis&quot;&gt;General Mediation Analysis&lt;/h2&gt;

&lt;p&gt;We could build an interface that encapsulates these chained linear regressions in this way:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;mediation&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mediation_analysis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;effects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mediation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;However, we could imagine a more abstract alternative:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;f1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f2&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mediation&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mediation_analysis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;effects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mediation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The advantage of the second approach is that it could in principle support any mediation or outcome model that the user provides, without our having to implement it internally. For example, in our microbiome application, we could try fitting a logistic-normal mediation model &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;f1 ~ lnm(m ~ t)&lt;/code&gt; to account for the ecosystem’s compositional structure.&lt;/p&gt;

&lt;p&gt;If we want to use such an interface, though, we will need a new way of computing direct and indirect effects, since we can’t simply read off \(\alpha, \beta_{m}\), and \(\beta_{t}\). Here is where the computational abstraction can inspire a statistical one: We can simulate forward through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;f1&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;f2&lt;/code&gt; to consider a variety of possible outcomes. Specifically, using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;f1&lt;/code&gt;, we can compute \(M\left(t\right)\), the value of the mediator under treatment \(t\). Similarly, using that output from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;f1&lt;/code&gt;, we can also simulate through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;f2&lt;/code&gt; to generate \(Y\left(M\left(t\right), t&apos;\right)\). Note that we can input different treatment values to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;f1&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;f2&lt;/code&gt; — \(t\) and \(t’\) do not need to be the same.&lt;/p&gt;

&lt;p&gt;Given this notation, a new way to define direct and indirect effects is:&lt;/p&gt;

\[\begin{align*}
Direct\left(t\right) &amp;amp;= \mathbf{E}_{\epsilon^{y}}\left[Y\left(M\left(t\right), 1\right) - Y\left(M\left(t\right), 0\right)\right] \\
Indirect\left(t\right) &amp;amp;= \mathbf{E}_{\epsilon^y, \epsilon^m}\left[Y\left(M\left(1\right), t\right) - Y\left(M\left(0\right), t\right)\right].
\end{align*}\]

&lt;p&gt;Notice that these depend on \(t\), so if we want an overall direct or indirect effect, we could average over \(t = 0, 1\).&lt;/p&gt;

&lt;h2 id=&quot;illustration&quot;&gt;Illustration&lt;/h2&gt;

&lt;p&gt;Let’s work through the microbiome example using this general interface to
mediation analysis. The block below reads in and preprocesses the data from the
&lt;a href=&quot;https://github.com/borenstein-lab/microbiome-metabolome-curated-data&quot;&gt;microbiome-metabolome-curated-data&lt;/a&gt;
repository (a great resource for multi-omics data analysis generated by Elhanan
Borenstein’s group at TAU):&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;Sys.setenv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;VROOM_CONNECTION_SIZE&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5e6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taxa&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_tsv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;https://raw.githubusercontent.com/borenstein-lab/microbiome-metabolome-curated-data/main/data/processed_data/FRANZOSA_IBD_2019/genera.tsv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metabolites&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_tsv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;https://raw.githubusercontent.com/borenstein-lab/microbiome-metabolome-curated-data/main/data/processed_data/FRANZOSA_IBD_2019/mtb.tsv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_tsv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;https://github.com/borenstein-lab/microbiome-metabolome-curated-data/raw/main/data/processed_data/FRANZOSA_IBD_2019/metadata.tsv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taxa&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taxa&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filter_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;simplify_tax_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metabolites&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metabolites&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filter_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;simplify_metab_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;combined&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metabolites&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bind_cols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taxa&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bind_cols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;treatment&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ifelse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Study.Group&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Control&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;control&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;treatment&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The full source code can be found
&lt;a href=&quot;https://rpubs.com/krisrs1128/mediation_software&quot;&gt;here&lt;/a&gt;. So that this process
doesn’t take too long, I’ll consider only the twenty most prevalent metabolites
and species.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;taxa_names&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colnames&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taxa&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metabolite_names&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colnames&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metabolites&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mediations&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;glue&lt;/code&gt; calls in the block below allow us to define the mediation and outcome
models for all pairs of metabolites and taxa under consideration.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;pb&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;progress_bar&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;[:bar] :percent eta: :eta&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;seq_along&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metabolite_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i_prime&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;seq_along&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taxa_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;formula&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;glue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;{metabolite_names[i]} ~ treatment&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;combined&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit2&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;formula&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;glue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;{metabolite_names[i]} ~ treatment + {taxa_names[i_prime]}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;combined&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pair&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;glue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;{metabolite_names[i]}_{taxa_names[i_prime]}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mediations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mediate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sims&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;treat&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;treatment&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mediator&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taxa_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i_prime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;control.value&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;control&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;treat.value&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;treatment&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pb&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tick&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Here is a volcano plot. The package uses a permutation test and occasionally
returns \(p\)-values of exactly zero. To ensure that we can still plot \(-log
p\), I’ve changed all those exact zeros into&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; \(p = 0.005\). There are direct effects that are clearly both practically and
statistically significant, and there even appear to be some promising indirect
effects.&lt;/p&gt;
&lt;figure&gt;
&lt;center&gt;
    &lt;img src=&quot;http://localhost:4000/images/mediation_software/volcano.png&quot; width=&quot;500&quot; /&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;Let’s look at these indirect effects more carefully. Here are some of the strongest effects.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_values&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;term&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Indirect(0)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;estimate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p.value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;estimate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;across&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;estimate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p.value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# A tibble: 91 x 6&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metabolite&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;taxon&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;                 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;term&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;estimate&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std.error&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p.value&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;             &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;                 &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;          &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dbl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dbl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;   &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dbl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C18_eg_Clser_0011&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Copromorpha&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;           &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Indirect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-2.69&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.398&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;   &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.005&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C18_eg_Clser_0011&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Faecousia&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;             &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Indirect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-2.65&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.396&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;   &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.005&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C18_eg_Clser_0018&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Faecousia&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;             &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Indirect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2.54&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.687&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;   &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.005&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C18_eg_Clser_0011&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Lachnoclostridium_B&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;   &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Indirect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-2.35&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.426&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;   &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.005&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C18_eg_Clser_0011&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Marvinbryantia&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Indirect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-2.18&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.442&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;   &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.005&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C18_eg_Clser_0011&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Anaeromassilibacillus&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Indirect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-2.15&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.335&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;   &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.005&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C18_eg_Clser_0018&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Lachnoclostridium_B&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;   &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Indirect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2.14&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.713&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;   &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.005&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The story for the first row gets a bit complicated, so let’s instead consider
the second – how is the metabolite &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C18_eg_Clser_0011&lt;/code&gt; mediated by the
abundance of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Faecousia&lt;/code&gt;? Here is the difference in the metabolite’s abundance
between treatment and control:&lt;/p&gt;
&lt;figure&gt;
&lt;center&gt;
    &lt;img src=&quot;http://localhost:4000/images/mediation_software/metabolite.png&quot; width=&quot;400&quot; /&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;And here is the relationship with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Faecousia&lt;/code&gt;.&lt;/p&gt;
&lt;figure&gt;
&lt;center&gt;
    &lt;img src=&quot;http://localhost:4000/images/mediation_software/mediation.png&quot; width=&quot;600&quot; /&gt;
&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;If we ignore the zeros, there does seem to be a clear linear relationship
between the metabolite and the abundance of this taxon. Moreover, IBD patients
seem to have noticeably fewer of these bacteria relative to the control
population. This is nearly a textbook example of an indirect effect — we’re
seeing how the change in the mediator result can explain a difference we
originally only saw in the outcome.&lt;/p&gt;

&lt;p&gt;A few of us in the lab are considering how to make this analysis even more
convincing:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Instead of filtering so aggressively, ideas from high-dimensional mediation
  analysis can be used to screen for the most promising pairs of species and
  metabolites. In this way, we can maintain power while maintaining
  computational tractability.&lt;/li&gt;
  &lt;li&gt;In this example, a linear outcome model is less satisfying than one that
  could account for zero inflation. More generally, both the linear and outcome
  models could include distributions that are commonly used in bioinformatics.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt;

&lt;p&gt;An alternative view of this discussion is that software principles can inspire
meta-algorithms.  Really, creating meta-algorithms is analogous to functional
programming – when we use
&lt;a href=&quot;https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.full&quot;&gt;boosting&lt;/a&gt;
or &lt;a href=&quot;https://people.eecs.berkeley.edu/~angelopoulos/publications/downloads/gentle_intro_conformal_dfuq.pdf&quot;&gt;conformal inference&lt;/a&gt;,
we have to first provide a base learning algorithm as input. Similarly, if we
are designing code for use by statistical programmers, it can be valuable to use
a functional style, since it makes the code naturally extensible. By letting
users freely define a base estimation approach (e.g., the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lm&lt;/code&gt; calls above),
meta-algorithms can have much wider problem-solving reach.&lt;/p&gt;

&lt;p&gt;A related point — when I’m creating projects these days, I try to be deliberate
about what the interface will look like to the practicing data analyst. This is
a departure from my past way of working, where I would have been happy just to
propose a new model. This perspective is helpful because it makes me think more
carefully about the resulting computational artifacts and how exactly they will
look like to the user. The hope is that the result from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;plot()&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;print()&lt;/code&gt;
should convey clear, qualitative value.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;I am being a lazy Bayesian. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Tue, 10 Oct 2023 19:27:00 -0500</pubDate>
        <link>http://localhost:4000/posts/mediation-software</link>
        <guid isPermaLink="true">http://localhost:4000/posts/mediation-software</guid>
      </item>
    
      <item>
        <title>CHTC for Statistics Crash Course</title>
        <description>&lt;p&gt;These notes collect some tips for using the &lt;a href=&quot;https://chtc.cs.wisc.edu/&quot;&gt;Center for High Throughput
Computing (CHTC) Cluster&lt;/a&gt; at UW Madison, especially
those related to statistics research. For reference, the full documentation is
available &lt;a href=&quot;https://htcondor.readthedocs.io/en/latest/&quot;&gt;here&lt;/a&gt;; there are many
more possibilities than we discuss here. You can follow along with the code in
&lt;a href=&quot;https://github.com/krisrs1128/chtc_crash_course&quot;&gt;this repository&lt;/a&gt; – let us
know if you find any mistakes or would like to share any other tricks!&lt;/p&gt;

&lt;h3 id=&quot;why-use-a-computer-cluster&quot;&gt;Why use a computer cluster?&lt;/h3&gt;

&lt;p&gt;There are three types of statistical computing problems where a cluster is
helpful,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Parallel Simulations: It is standard in statistics research to understand how an algorithm behaves across a range of data or hyperparameter regimes (e.g., noise levels, regularization parameters). While in principle these experiments can all be run sequentially on a laptop, whenever there are a large number of configurations to consider, it will save time to run them in parallel. In theory, processes can be parallelized on a single laptop. Practically, however, only a few processes can be run at a time without overloading a typical machine. On a computer cluster, we can run many simultaneous jobs without any issues.&lt;/li&gt;
  &lt;li&gt;Long-Running Jobs: Sometimes we only need to run one script, but it takes a long time to complete. There have been times where I’ve kept a laptop running continuously for a few days, but this is always a little nerve-wracking and can distract from other tasks. If the long-running job is run on a CHTC machine instead, we can check on it whenever it’s complete without having it always visible in the background. Also, when more resources are available, the job will likely finish more quickly.&lt;/li&gt;
  &lt;li&gt;GPU Jobs: Most of us don’t have GPU cards on our personal computers. CHTC has a great GPU Lab, where we can checkout a GPU whenever we need one.&lt;/li&gt;
  &lt;li&gt;Large Datasets: There are some datasets that we can’t store on our laptops. A reasonable workflow is to first develop code on a subset of the data on a personal machine. Once we trust that our code works on the subset, we can attempt running on the full dataset using CHTC.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;logging-onto-the-cluster&quot;&gt;Logging onto the cluster&lt;/h3&gt;

&lt;p&gt;After   &lt;a href=&quot;https://chtc.cs.wisc.edu/uw-research-computing/form&quot;&gt;requesting&lt;/a&gt; an
account from CHTC, you can login to a remote node using&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ssh yournetid@submit2.wisc.edu&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Note that if you are off campus, you will first need to login to the
&lt;a href=&quot;https://kb.wisc.edu/helpdesk/page.php?id=105619&quot;&gt;GlobalProtect VPN&lt;/a&gt;. Otherwise,
the cluster machines will not be reachable.&lt;/p&gt;

&lt;h4 id=&quot;tip-aliases&quot;&gt;Tip: Aliases&lt;/h4&gt;

&lt;p&gt;You can create an alias to login to the cluster more quickly. On your personal
machine, you should have a folder &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.ssh/&lt;/code&gt; containing a few public and private
key files. If you &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scp&lt;/code&gt; the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;id_rsa.pub&lt;/code&gt; public key into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.ssh/&lt;/code&gt; folder on the
remote machine, then you will be able to login using an alias after you,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;chmod 600&lt;/code&gt; the remote copy to ensure that it is not too visible.&lt;/li&gt;
  &lt;li&gt;Add the following lines to your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.ssh/config&lt;/code&gt; on your local machine.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;host your_alias
  HostName submit2.chtc.wisc.edu
  User your_netid&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;For example, I set my alias to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;chtc&lt;/code&gt;, which lets me login to the cluster using
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ssh chtc&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Once you log in, you can browse around and view a few files – you should have
access to create files in a folder like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/home/your_net_id&lt;/code&gt; and
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/staging/your_net_id&lt;/code&gt;. The first folder is where routine code can be kept. The
second is where data should be placed just before launching a job (and remove
shortly afterwards).&lt;/p&gt;

&lt;h3 id=&quot;setting-up-a-simple-job&quot;&gt;Setting up a simple job&lt;/h3&gt;

&lt;p&gt;A CHTC job is specified by a submit file. It describes the compute resources,
code, data, and logging instructions needed to complete the task. It is
essentially a dictionary written as a text file, whose key-value pairs conform
to conventions set by CHTC. Next, we’ll review some of the most commonly used
keys. Examples using these keys in real compute jobs are available
&lt;a href=&quot;https://github.com/krisrs1128/chtc_crash_course&quot;&gt;here&lt;/a&gt; and discussed further
below.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Specifying resources&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Computer clusters allow us to checkout more resources than we would have on our
own. To keep it from becoming a free-for-all, it’s necessary to estimate how
much resources we need to complete a task (the more we ask for, the longer we
have to wait before the job can start).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;request_memory&lt;/code&gt;: How much memory does the job need? This is usually determined by the size of the largest matrix or data.frame that needs to be loaded into R or python. Example values are &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;16GB&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;500MB&lt;/code&gt;. You can request much more memory than we have on a laptop – asking for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;128GB&lt;/code&gt; of memory would be a lot but not totally out of the ordinary.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;request_disk&lt;/code&gt;: How much disk space do we need for storing data and saving outputs? This is usually determined by the size of the dataset that is being processed, though it can also depend on the size of the saved output / logs. Note that a job can require a lot of disk but only a little memory – it may work off of a large dataset, but only processes small batches at a time. Typical values are &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;64GB&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;12GB&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;request_cpus&lt;/code&gt;: If you have a process that can utilize several cores, then it can help to make use of several cpus. By default, though, I just set this to 1, since most of the parallelization can happen at the cluster job level.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;queue&lt;/code&gt;: How many parallel jobs should we run? More on parallelization below.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Specifying code&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Cluster code needs to be more or less automatically runnable; we can’t rely on
interactively sending commands one after another like we might on our own
laptops. We also need to ensure that the environment on the cluster compute node
is close enough to our local environments; i.e., all packages and environmental
variables that we require locally need to be accessible remotely.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;executable&lt;/code&gt;: This gives the name of a shell script containing all the commands to be run in the CHTC job. If we weren’t using a computer cluster, it would be the only script that needs to be run.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;environment&lt;/code&gt;: We may want to use environmental variables within the executable. This is especially useful when looping over parameter values in a parallel array of jobs.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker_image&lt;/code&gt;: If we need packages that are hard to directly install on CHTC, it can be convenient to build a docker image on your local machine and run the CHTC job on a public version of that image. Note that for this key to be read, we first need to specify &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;universe=docker&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Specifying data&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Whatever data we work with locally needs to be accessible on the remote machine.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;transfer_input_files&lt;/code&gt;: The absolute or relative path to a file (or comma separated list of files) that should be transferred to the compute node when a job begins. Note that you can also transfer code repositories using this command. I typically zip everything I need into a single file (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tar -zcvf all_files.tar.gz all_files/&lt;/code&gt;) and then point the argument to this file. Note that all files in the same directory as the submit file will automatically be transferred to the working directory of the compute job.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;requirements = (Target.HasCHTCStaging == true)&lt;/code&gt;: Instead of directly transferring input files, we can keep our files unzipped in a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/staging&lt;/code&gt; directory and then copy them over within the executable script. For this approach to work, though, the compute node needs access to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/staging&lt;/code&gt; filesystem. This flag ensures this access is supported (not all compute nodes can access &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/staging&lt;/code&gt;). Note that several requirements can be changed together using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(requirement 1) &amp;amp;&amp;amp; (requirement 2)&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Specifying logs&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Logs are helpful for debugging. They also tell how much resources a job computed after-the-fact, which can be helpful for updating resource requests in future jobs. Note the use of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$(Process)&lt;/code&gt; in the filenames below – this makes sure that log files are not overwritten in parallel runs.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;error&lt;/code&gt;: The name of the error file.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;output&lt;/code&gt;: The name of a file that will include any messages printed out by the executable script.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log&lt;/code&gt;: This name of a file describing how many resources are used by the job.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Putting this all together, here is an example submit file,&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nv&quot;&gt;universe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;docker
&lt;span class=&quot;nv&quot;&gt;docker_image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;ubuntu/focal

&lt;span class=&quot;nv&quot;&gt;executable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;basic.sh
&lt;span class=&quot;nv&quot;&gt;transfer_input_files&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;file1.csv,file2.csv

&lt;span class=&quot;c&quot;&gt;# requesting resources&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;request_memory&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;10MB
&lt;span class=&quot;nv&quot;&gt;request_cpus&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
&lt;span class=&quot;nv&quot;&gt;request_disk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1MB

&lt;span class=&quot;c&quot;&gt;# log files&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;basic-&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;Process&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;.err
&lt;span class=&quot;nv&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;basic-&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;Process&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;.out
&lt;span class=&quot;nv&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;basic-&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;Process&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;.log
queue&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;and this is the accompanying executable.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-lsh&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;wc &lt;/span&gt;file&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sleep &lt;/span&gt;30s&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;These files are downloadable
&lt;a href=&quot;https://github.com/krisrs1128/chtc_crash_course/tree/main/basic_job&quot;&gt;here&lt;/a&gt;. The
job can be submitted to the cluster using the command,&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;condor_submit basic.submit&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;What do you think is output by the job?&lt;/p&gt;

&lt;p&gt;Finally, we note that any of the parameters specified in the submit file can be
overwritten at submit time as an argument to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;condor_submit&lt;/code&gt;. This is useful
whenever several similar executables can be run using the same background
resources and input. For example, we can use&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;condor_submit train.submit &lt;span class=&quot;nv&quot;&gt;executable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;model1.sh
condor_submit train.submit &lt;span class=&quot;nv&quot;&gt;executable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;model2.sh&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;to run two separate models,&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model1&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model2&lt;/code&gt;, using the same resources,
specified by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train.submit&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;debugging-a-compute-job&quot;&gt;Debugging a Compute Job&lt;/h3&gt;

&lt;p&gt;It would be a miracle if a compute job worked on its first submission. More
commonly, we submit small preliminary versions of a job (requesting fewer
resources) until we are confident that the script works. The faster we can debug
errors, the sooner we can get results.&lt;/p&gt;

&lt;p&gt;The first thing to do when debugging a failed cluster job is to view the log
files. Errors that stopped the compute process will be saved in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;error&lt;/code&gt; log
specified in the submit script, but useful hints are often available in the
messages saved in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;output&lt;/code&gt; file. If you are running a parallel job, it might
be useful to read the last few lines of many error logs. For example, the command&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;tail&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; 20 &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;.err&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;will print the last twenty lines of every error log in a directory.&lt;/p&gt;

&lt;p&gt;Besides analyzing the logs and resubmitting a cluster job, here are some
utilities that streamline the process,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;condor_q&lt;/code&gt;: This shows all the current jobs, their IDs, and whether or not they are running.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;condor_submit -i&lt;/code&gt;: This runs an interactive version of a submit file. Once resources are checked out, you will have interactive access to an environment that looks exactly like what the original job’s environment would look like. If every line of the executable works in an interactive job, then it will also work in a non-interactive submission.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;condor_ssh_to_job&lt;/code&gt;: This will let you log into a currently running compute node and watch the directory as the process is running. This is especially useful if a job doesn’t fail right away but you know there are certain outputs that indicate the job will eventually fail. This way, you can check the output and kill a doomed job without waiting for the process to officially fail.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;condor_rm&lt;/code&gt;: If we realize a bug and want to remove a job, we can call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;condor_rm job_id&lt;/code&gt;, using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;job_id&lt;/code&gt; printed by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;condor_q&lt;/code&gt;. If we want to remove all the jobs, we can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;condor_rm --all&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;condor_q -hold -af HoldReason&lt;/code&gt;: If our job is being held for some reason (e.g., we have a typo in the input files or ran out of memory), we can inquire into the reason using this command, which takes the job’s ID as its argument.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, if we had asked CHTC to run using a docker environment, we can simulate
the CHTC environment using on our local machine. This works because the Docker
VM is  identical in both computers. However, CHTC doesn’t use the ordinary
Docker run command, so the filesystem and permissions will be slightly
different. To account for the discrepancy, we can use&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker run &lt;span class=&quot;nt&quot;&gt;--user&lt;/span&gt; &lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-u&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;:&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-g&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt;   &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; &lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pwd&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;:/scratch &lt;span class=&quot;nt&quot;&gt;-w&lt;/span&gt; /scratch   YOUR_DOCKER_IMAGE_ID /bin/bash&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;locally to exactly mimic the CHTC environment.&lt;/p&gt;

&lt;h3 id=&quot;pattern-1-parallel-runs&quot;&gt;Pattern 1: Parallel Runs&lt;/h3&gt;

&lt;p&gt;One of the most common patterns in statistics research is to launch a large
number of relatively small jobs (15 - 120 minutes) to see how an algorithm
varies over different dataset or hyperparameter configurations (e.g., matrix
aspect ratios, latent dimensionalities, noise levels, or regularization
parameters). This is easily supported by the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;queue&lt;/code&gt; parameter in CHTC jobs. The
main idea is,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Write code that takes data or algorithm parameters as arguments.&lt;/li&gt;
  &lt;li&gt;Create a table with all the parameter configurations of interest.&lt;/li&gt;
  &lt;li&gt;Request a number of jobs equal to the number of desired configurations.&lt;/li&gt;
  &lt;li&gt;For the $i^{th}$ job, read in the $i^{th}$ row of the table and use it to provide arguments to the generic code above.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, suppose we have a table with 5 configurations,&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;A,B,C
1,2,3
2,3,4
5,6,7
8,9,10
11,12,13
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;then we can queue 5 jobs and pass each job’s number in the queue as an
environmental variable. Specifically, the submit file, we add&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nv&quot;&gt;transfer_input_files&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;configurations.csv
environment &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;id=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;Step&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
queue 5&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;and we refer to the ID environmental variable in the executable,&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;Rscript &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;rmarkdown::render(&apos;parallel.Rmd&apos;, params=list(id=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$id&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;))&quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;which is designed to read the associated row from the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;configurations.csv&lt;/code&gt;. This
command used an R process, but the same idea works in python or julia with the
appropriate argument parsers.&lt;/p&gt;

&lt;p&gt;A full example of this pattern can be found
&lt;a href=&quot;https://github.com/krisrs1128/chtc_crash_course/tree/main/parallel_job&quot;&gt;here&lt;/a&gt;.
Note that the submit script pulls from a &lt;a href=&quot;https://hub.docker.com/r/rocker/tidyverse&quot;&gt;rocker
image&lt;/a&gt;, because the executable needed
to run R with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rmarkdown&lt;/code&gt; package installed.&lt;/p&gt;

&lt;h2 id=&quot;pattern-2-gpu-jobs&quot;&gt;Pattern 2: GPU Jobs&lt;/h2&gt;

&lt;p&gt;GPU jobs are almost exactly like ordinary jobs, except that they need to
explicitly request nodes with GPUs. This is done using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;request_gpu&lt;/code&gt; flag.
We usually set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;request_gpu=1&lt;/code&gt;, though it’s also possible to run a job with
multiple GPUs (only do this if your code actually knows how to load data into
several GPUs, though). Some other useful commands are,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;+GPUJobLength&lt;/code&gt;: Will the job be done in a couple of hours? Will it need more than 24? This can be set to either &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;short&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;medium&lt;/code&gt;, or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;long&lt;/code&gt; to ensure that the job won’t be kicked out too early.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;+WantGPULab&lt;/code&gt;: This is a shared GPU resource that isn’t included by default in CHTC jobs. This is almost always set to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt;, since it helps jobs launch more quickly.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;+wantFlocking&lt;/code&gt;: This is another shared GPU resources, like the GPU Lab. This is also usually set to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some code will work on some GPUs and not others (e.g., a run with large batch
size might need extra GPU memory). We can restrict the GPUs that are considered
for a job by using the same &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;requirements&lt;/code&gt; argument described above. For
example,&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;requirements &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;CUDAGlobalMemoryMb &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; 4000&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;will only run jobs on compute nodes with at least 4GB GPU memory. To see whether
a request has a chance of being fulfilled in the near future (or ever…if you
are like me and request resources that do not exist), we can review the
cluster’s GPU resources,&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;condor_status &lt;span class=&quot;nt&quot;&gt;-compact&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-constraint&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;TotalGpus &amp;gt; 0&apos;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Additional constraints can be used to filter this list. For example, to see
nodes with at least CUDA 6.0  installed and 12GB of memory, we could use&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;condor_status &lt;span class=&quot;nt&quot;&gt;-compact&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-constraint&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;TotalGpus &amp;gt; 0&apos;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-constraint&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;CUDACapability &amp;gt; 6&apos;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-constraint&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;CUDAGlobalMemoryMb &amp;gt; 12000&apos;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;A full example of this pattern can be found
&lt;a href=&quot;https://github.com/krisrs1128/chtc_crash_course/tree/main/gpu_job&quot;&gt;here&lt;/a&gt;. The
script runs a file called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gpu.py&lt;/code&gt;, which is only a few lines,&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cuda&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;but this is enough to see that we can put the tensor &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x&lt;/code&gt; onto a GPU. Indeed, the
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;output&lt;/code&gt; file confirms that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x&lt;/code&gt; had been on the GPU,&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;cuda:0&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

</description>
        <pubDate>Thu, 07 Oct 2021 15:56:24 -0500</pubDate>
        <link>http://localhost:4000/posts/chtc-crash-course</link>
        <guid isPermaLink="true">http://localhost:4000/posts/chtc-crash-course</guid>
      </item>
    
      <item>
        <title>'What is a sample, even?' and the Existential Questions of Data Wrangling</title>
        <description>&lt;p&gt;In your life as a data scientist, you will spend a lot of time trying to
generate new artifacts from “raw” data. These artifacts are tangible things,
living on a device, a website, and yes, maybe even on paper, including things
like,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Model predictions&lt;/em&gt;, which are usually a component of some (potentially
 automated) decision making process. Some examples are predictions about which
 song to recommend someone (automated) or which educational interventions to
 give to struggling students (not automated).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Model summaries&lt;/em&gt;, which inform our understanding of the data generating’s
 systems underlying mechanics. Some examples might be quantitative measures
 about whether living in Montreal influences your inherent interest in heavy
 coats or whether smoking causes lung cancer.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Figures&lt;/em&gt;, which communicate salient patterns in a dataset, to inform
 understanding or decision-making&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. For example, should you &lt;a href=&quot;https://en.wikipedia.org/wiki/File:Minard.png&quot;&gt;invade
 Russia&lt;/a&gt;?&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Reports&lt;/em&gt;, which synthesize of a few parts of a data analysis to provide
 evidence for some higher-level theory.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;No matter what your downstream artifact will be, you’ll need to do a bit of work
upstream, mainly around,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Problem Formulation: What are you trying to do, and why does it matter?&lt;/li&gt;
  &lt;li&gt;Data Wrangling: How does the data need to be stretched, squeezed, and scrubbed
to make it suitable for automatic, algorithmic processing.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;From the previous lecture / rant, we know that data are diverse, so it’s no
surprise that the answers to these questions will be very context dependent.
That said, I find a few general principles relevant in almost any application.
The principles for (1) are profound, we’ll hopefully revisit them later. But (2)
is within reach even at this point, so let’s focus on that.&lt;/p&gt;

&lt;p&gt;In the abstract, we’d love to think of our data as a big matrix \(X\) with \(n\)
rows and \(p\) columns. The rows correspond to samples and the columns
correspond to attributes. The \(ij^{th}\) element of this matrix will be
\(x_{ij}\), we’ll often write the \(i^{th}\) row as just \(x_i\).&lt;/p&gt;

&lt;p&gt;For example, each row might be a person in this class. The first column might be
name, the second might be major, and the third might be favorite pokemon. Or,
the samples might be photos of handwritten digits, and the columns might measure
intensity at different &lt;a href=&quot;https://observablehq.com/@mbostock/lets-try-t-sne&quot;&gt;pixels&lt;/a&gt;&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Unfortunately, there are all sorts of obstacles that keep us from building this
beautifully mathematical \(X\). Let’s consider them one at a time, and what you
might be able to do about it.&lt;/p&gt;

&lt;h2 id=&quot;i-have-no-idea-what-these-columns-are&quot;&gt;I have no idea what these columns are&lt;/h2&gt;

&lt;p&gt;This isn’t strictly a barrier to putting everything into a single \(X\), but if
you are confused by this, you should stop coding and write to the people who
shared the data with you. They should be able to give you a data dictionary. You
don’t want to try doing anything to the data if you’re not even sure what they
are.&lt;/p&gt;

&lt;h2 id=&quot;managing-column-types&quot;&gt;Managing column types&lt;/h2&gt;

&lt;p&gt;Different columns will have different types, for example,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Numeric&lt;/li&gt;
  &lt;li&gt;(Ordered) categorical&lt;/li&gt;
  &lt;li&gt;Dates&lt;/li&gt;
  &lt;li&gt;(Positive) Integer&lt;/li&gt;
  &lt;li&gt;Text&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;and you want to make sure that the types stored in the computer reflect your
conceptual understanding of what that column should contain. For an example of
the type of conceptual mismatch that could cause trouble later on, the computer
might accidentally read in a numeric column as a string, because some entries
were sloppily recorded as strings – looking at the raw input file might reveal 
that the “height” variable has entries like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(..., 10.1, 9.2, forgot measuring
stick, 10.9, ...)&lt;/code&gt;. In this case, you’ll want to convert the string to a missing
value and coerce the column to a numeric.&lt;/p&gt;

&lt;p&gt;Dates can be annoying, but a bit of fiddling with
&lt;a href=&quot;http://strftime.net/&quot;&gt;strftime&lt;/a&gt; will usually get you what you need. Depending
on the task, it can be helpful to isolate some features from the dates – was
the date on a weekend, a holiday, or during rush hour?&lt;/p&gt;

&lt;p&gt;Categorical variables deserve some additional commentary, since they can be
especially tricky. First off all, you might notice that different categories are
actually referring to the same thing. One of the categories might be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;POTATO&lt;/code&gt;,
while another reads &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;potato&lt;/code&gt;. Unless you go in and consolidate these, they will
be treated as different categories in downstream analysis.&lt;/p&gt;

&lt;p&gt;Second, there are often many very rarely seen categories. It often makes sense
to lump these all into an “other” category.&lt;/p&gt;

&lt;p&gt;It may also be useful to split a categorical variable that encodes several pieces
of information into different columns.&lt;/p&gt;

&lt;p&gt;You might want to rename the categories (and sometimes column names) for the
sake of readability – really long column names can be a pain. That said, if
it’s going to take forever to do more than a few automatic conversions (e.g.,
replacing spaces, converting to lower case), it may not be worth the time.&lt;/p&gt;

&lt;p&gt;Finally, you may want to k-hot encode your factors. This is usually not needed
for visualization, but ML algorithms often need \(X\) to be a completely numeric
matrix.&lt;/p&gt;

&lt;h2 id=&quot;missing-values&quot;&gt;Missing values&lt;/h2&gt;

&lt;p&gt;You want to make sure that missing values are properly documented. This means
replacing things like “” and -999999 with the appropriate NA.&lt;/p&gt;

&lt;p&gt;It is sometimes also a good idea to distinguish between structurally and
randomly missing values. Structurally missing values are entries that are not
recorded for some reason that applies equally to many entries, e.g., a question
might not have been asked on a survey in one of the years. Randomly missing
values are isolated missing values, whose reasons for being missing are not
clearly characterized.&lt;/p&gt;

&lt;h2 id=&quot;joining-and-reorganizing&quot;&gt;Joining and Reorganizing&lt;/h2&gt;

&lt;p&gt;Often, the thing you want to think about as a unified \(X\) lives in a tangled
web of inconsistent files, in subdirectories that seem to have been nested for
no particular reason other than to torment the data scientist who was foolhardy
enough to think that the data could be properly analyzed. Actually, there are
often good reasons for this incoherence, including&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Databases: The data may have lived in a relational database, where rows in
different tables corresponded to different conceptual units. Hence, you are
given one table where each row describes a school, and another where each row
is a student. If you want to see characteristics of a student’s school when
analyzing their test results, you’re going to have to join these tables
together somehow.&lt;/li&gt;
  &lt;li&gt;Streaming-ness: The data may have been automatically generated by some
platform. It might spit out a different record every minute, say. You’ll want
to combine these as rows \(x_i\) of a newly formed matrix \(X\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A few caveats are in order. There first is that the data may be of different
types, and it might not make sense to join them. For example, you may have a
collection of satellite images, along with metadata associated with them (date
taken, geographic coordinates, sensor used). In this case, I recommend putting
the “nontabular” signals (images, audio recordings, …) into a directory and
including that path in a processed, metadata file.&lt;/p&gt;

&lt;p&gt;A related caveat is that the data might be too big to merge into one file –
this is especially the case with streaming data or data that you only ever
intend to handle in small batches at a time. Nonetheless, you still need to do
this wrangling, if only to (a) ensure consistency across the pieces and (b)
build a metadata file linking the pieces.&lt;/p&gt;

&lt;p&gt;The most profound exception to the recommendations above is that what we think
of as a single sample might vary from one analysis goal to another. Indeed, the
whole idea of a “sample size” is fraught, we’ve been lying to you all along so
that you don’t have an existential crisis too early.&lt;/p&gt;

&lt;p&gt;The issue often arises when data are not i.i.d. or have some sort of multiscale
structure. For example, in a longitudinal study, you might have tracked the same
few people for many years, recording information about them at particular
intervals. You have a choice here: each row \(x_i\) might include all the data
you ever captured about person \(i\). Alternatively, it might be reasonable to
split each person’s data across several rows, each corresponding to a timepoint.&lt;/p&gt;

&lt;p&gt;Another example is this bikesharing data. Maybe you should group the 24h periods
into rows, or maybe you should consider each timepoint separately.&lt;/p&gt;

&lt;p&gt;There’s no universal rule, what you do should depend on your analysis goal (and
potentially on the constraints imposed by your computing environment).&lt;/p&gt;

&lt;h2 id=&quot;final-points&quot;&gt;Final Points&lt;/h2&gt;

&lt;p&gt;When you go through these wrangling steps, you’ll be changing your original
data. For this reason, I urge you to always keep the raw data untouched, and
write the results of this wrangling to new processed files. These processed
files are what your algorithmic and inferential code will be touching
downstream.&lt;/p&gt;

&lt;p&gt;While I’ve listed some of my best practices, all sorts of unexpected things can
happen. I’ve seen excel footnotes mess up numerical columns, tables which
almost, &lt;em&gt;but not quite&lt;/em&gt; joined, and survey results from 2099.&lt;/p&gt;

&lt;p&gt;So, while “Let \(X \in \mathbb{R}^{n \times p}\)” might be the first line in
your statistics or ML textbook, just getting to this point can involve quite a
bit of work in any real-world data science problem. Don’t be deterred though –
the problems you can solve if you have the ability to sift through real-world
data are ultimately the most interesting and impactful ones.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;They might even guide whether to collect different types of data. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;See the section on “Images.” &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Thu, 29 Aug 2019 15:35:00 -0500</pubDate>
        <link>http://localhost:4000/posts/wrangling</link>
        <guid isPermaLink="true">http://localhost:4000/posts/wrangling</guid>
      </item>
    
      <item>
        <title>Data are not oil, they are cupcakes</title>
        <description>&lt;hr /&gt;

&lt;p&gt;By this point, it’s a bit cliche to say that data is
“&lt;a href=&quot;https://fortune.com/2016/07/11/data-oil-brainstorm-tech/&quot;&gt;the&lt;/a&gt;
&lt;a href=&quot;https://hackernoon.com/data-is-the-new-oil-1227197762b2/&quot;&gt;new&lt;/a&gt;
&lt;a href=&quot;https://www.economist.com/leaders/2017/05/06/the-worlds-most-valuable-resource-is-no-longer-oil-but-data/&quot;&gt;oil&lt;/a&gt;&lt;a href=&quot;https://medium.com/@amitgarg/data-is-the-new-oil-ai-is-the-new-electricity-who-is-building-the-new-railroads-c8995faa8fd2&quot;&gt;.&lt;/a&gt;”
It’s the sort of phrase you might have heard if you didn’t have headphones on
when you went to that one &lt;a href=&quot;https://tinyurl.com/y53g79n6&quot;&gt;hip cafe&lt;/a&gt; the other
day. And while some parts of the metaphor are relevant (“&lt;a href=&quot;https://fr.wikipedia.org/wiki/Scandale_Facebook-Cambridge_Analytica&quot;&gt;Data
spill&lt;/a&gt;” has
kind of a ring to it), other parts are potentially misleading, especially for an
aspiring data scientist.&lt;/p&gt;

&lt;p&gt;Instead, I propose that we think of data as cupcakes, or really, desserts more
generally. There are some reasons,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Data are the result of human intervention. Data don’t just exist in the
ground because dinosaurs died a long time ago (though, sticklers might
&lt;a href=&quot;http://dinosaurpictures.org/&quot;&gt;disagree&lt;/a&gt;). Someone has to deliberately place
a sensor somewhere, or track a user’s browsing behavior, or run a sample
through some device, or launch a camera into space…. and then write down
everything that the sensor saw, because they think that information might be
useful for some reason someday.&lt;/p&gt;

    &lt;p&gt;Of course, this is just like cupcakes. Cupcakes don’t appear out of thin air
(we can dream), they are deliberately constructed for human consumption.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Data are wildly varied across domains. This is a corollary of point 1. Since
data are generated by people, and since people have extraordinarily diverse
backgrounds and intentions, the data you will encounter as a professional
data scientist will vary dramatically from project to project. Museum
directors, city planners, ecologists, and healthcare providers are all
collecting data these days – I think it’s a small miracle that a few of our
mathematical abstractions (randomness, latent structure, inference) are
relevant across such broad domains.&lt;/p&gt;

    &lt;p&gt;Oil is oil no matter where you go, desserts come in
&lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_Turkish_desserts&quot;&gt;many&lt;/a&gt;
&lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_Argentine_sweets_and_desserts&quot;&gt;different&lt;/a&gt;
&lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_Chinese_desserts&quot;&gt;flavors&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We can strengthen point 2, if we note that, even within a single application
domain, the data are not uniform. It’s now standard for a genomics data
analysis to incorporate measurements from many different modalities. Formal
government surveys are often complemented by automatically recorded
transactions. On the internet, images, text, temporal, and relational
information are all baked together into a delicious, fruit-filled, and
icing-topped whole, as you will discover in your course project.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Okay, okay, I know what you’re going to say, “But Kris, cupcakes have never
jeopardized civil society” and “all these additional dessert regulations are
going to stifle &lt;a href=&quot;https://fr.wikipedia.org/wiki/Cake_Boss&quot;&gt;innovation&lt;/a&gt;.” Yes,
this metaphor has it’s own limits, but I hope I’ve gotten my main point across,
which is that data are human artifacts, generated by people with diverse visions
of the future.&lt;/p&gt;
</description>
        <pubDate>Thu, 29 Aug 2019 15:35:00 -0500</pubDate>
        <link>http://localhost:4000/posts/cupcakes</link>
        <guid isPermaLink="true">http://localhost:4000/posts/cupcakes</guid>
      </item>
    
      <item>
        <title>Teaching attention, part 5 / N</title>
        <description>&lt;hr /&gt;

&lt;p&gt;The slides are &lt;a href=&quot;https://observablehq.com/@krisrs1128/untitled/4&quot;&gt;here&lt;/a&gt;. Each —
marks a new slide.&lt;/p&gt;

&lt;p&gt;####&lt;/p&gt;

&lt;p&gt;I have this recurring daydream, where I wish I could capture the entire feeling
of a moment, without having to try. I feel this sometimes when I walk through a
garden, and I try to remember forever exactly how the reflections off the water
looked, the patterns of light and shadow on the leaves.&lt;/p&gt;

&lt;p&gt;I sometimes feel the same way about data. What if I could understand a raw
dataset at a glance? If I were in a competition to identify farmland from
satellite imagery, I’d just look the test set and all the coordinates would
magically appear in my head.&lt;/p&gt;

&lt;p&gt;This unfortunately isn’t possible, but not everything is lost. I mean, we can’t
breathe underwater, but we’ve somehow mapped out the ocean floor.&lt;/p&gt;

&lt;p&gt;In the same way, people can’t just look at data and make sense of them, but
we’ve made a science out of it. It may not be completely effortlesss, but in its
own way, I think it’s magical.&lt;/p&gt;

&lt;p&gt;In this lecture, we’ll study some of the fundamental ways we can take complex
information, and process it in a way so that it makes sense.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;A good mathematical analogy for this process of compressing information is coin
flipping. The binomial distribution is a kind of slacker genius, all these
outcomes can be flying at it, but it ignores everything except the proportion of
outcomes that come up heads. But you can’t blame it, because that’s all you need
in order to solve downstream inference tasks.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;We need to cary this idea further, this idea that you should capture what is
essential, and throw away the rest. Fortunately, there are whole theories built
around it. In statistics, it’s called sufficiency, and in deep learning it’s
called memory and attention. No matter what you call it, the basic point is the
same: good models compress data, whether it’s coin flips into the proportion of
heads, or half a terabyte of satellite imagery into a several megabyte neural
network.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Next, recurrent neural networks.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;RNN cells give a way for computing summaries from streams of information. In a
way, they are some of the most fundamental units available for the task. h[t -
1] is our previous summary, then we observe x[t], and based on that new data, we
update our summary into h[t]. Visually, the h’s are these blocks, which are
continually being updated every time a red input x arrives.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;This has a direct analogy to the situation with coin flips. Here, the summaries
are estimates of the probability of coming up heads, and the inputs x are the
observed coin flips.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Coin flips are nice, but they are discrete. I want to show you an example with
continuous data, so let’s consider the related task of counting how many times a
time series crosses the interval [0, 1]. It’s similar to counting the number of
heads so far, but the raw inputs are no longer binary.&lt;/p&gt;

&lt;p&gt;If I move the series around, the summary should update to reflect the new
interval counts. (play with the figure)&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;It turns out that we can completely define a processor that computes this
summary, using a two dimensional \(h\). The idea is that to find the number of
entrances into the interval, we just need to track whether the series is in the
interval at the current timestep, but wasn’t at the previous one. However, our
processor can only look at the current input (not the previous one). We can get
around this by realizing that we can refer to earlier summaries (we can look at
h[t - 1] when finding h[t]), so we’ll just store the previous raw input as the
first coordinate of the summary.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;This is a mouthful, but the figure below makes it clearer.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;I hope you realize how amazing this is. We’ve taken something that’s global, and
reduced it to a series of local calculations. This is how memory works. You look
at lots of small pieces, one at a time, and combine them into a coherent memory,
something that makes sense holistically.&lt;/p&gt;

&lt;p&gt;That said, you should be a little disappointed with this approach. We had to
handcraft our processors, which goes against the whole philosophy of deep
learning. It’s like the picture on the left, we’ve defined some complicated
functions during each of the processing steps. They get the job done, but
required thought.&lt;/p&gt;

&lt;p&gt;Instead, let’s try using the deep learning idea: using compositions of simple
functions, plus a little bit of supervision, to automatically learn the best
processors. It’s like this picture on the right. We’ll pass the input through
many layers, hoping that the combination of simple functions becomes an
interesting, complex one.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;What should these simple, composable units look like? The answer isn’t too
surprising: each layer is a linear function followed by a nonlinearity. The
input enters through this term, the previous summary enters through this. For
two-dimensional h’s and x’s, we can visualize this directly. The previous
summary are the yellow points, and the corresponding update would be the purple
h’s. The weight matrices are here in the bottom right. Let’s see what happens
when I change the input. (move the input). Does anyone know why it didn’t move
anything?&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Okay, so those are the units. To learn to compose them in meaningful ways, we
need to provide supervision. There are a few different ways we could do this.
One approach is to provide a scalar output associated with each sequence. This
is like sentiment analysis. The input is a sequence of words in a review, the
output is a label for whether the review is positive or negative.
An alternative is to supervise each term in the input. This is like in a general
language model – your goal might be to predict the next word, given the
sequence so far. Finally, your output might itself be a sequence. This is like
in Dr. Walcott Brown’s talk, where the input was a medical report, and the
output is a sequence of words giving a summary.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;In principle, this should be enough for training an RNN. However, as is, these
models tend to suffer from something called the vanishing gradient problem. The
idea is that since I’m making a small update at every time point, the influence
of an early point on the current summary diminishes. The RNN has a memory, but
it’s not very long term.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;We can also see this visually. Clicking here creates a new input. The new point
here is the updated summary. When I create a long sequence of inputs, the units
end up saturating, and the summaries collapse. Changing an input early in the
sequence has little effect on the summary near the end. This is exactly the
vanishing gradient problem – gradients are sensitivity of a value to a
perturbation, and if the perturbation leads to very little change, the gradient
must be small.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;To get around this, and get truly long-term memories, we need to introduce
something called gating. These gates keep useful information from getting
overwritten. To be honest, I was probably a little sleep deprived when I added
this figure, but the point was that these gates are keeping undesirable
information from entering.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;There are a few different ways to implement gating, but one popular approach are
GRUs, gated recurrent units. We introduce variables z[t] and r[t] that control
how much the old h and new x can possibly influence the new summary. A large
value of z prevents the old summary from contributing much to the new h. A large
value of r means that x has somewhat less influence on the new h, compared to
the previous h.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;This has a nice geometric interpretation, if we go back to the figure. z[t]
controls how much h[t] can change between time units. (show this)
r[t] controls how much x[t] matters, relative to h[t - 1]. If you set it to 0,
it doesn’t matter what the previous h was, it pays attention only to the input
value.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;With gating, you’ll now be able to train an RNN that learns sophisticated,
long-term features. We’re now able to directly learn the counter function that
we had hard-coded early on, just by providing lots of examples.&lt;/p&gt;

&lt;p&gt;You can see that the model had no idea for the first few epochs, but eventually
realizes which points have lots of crossings, and which have none.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;We can also inspect the features that were learned. Each row here is one of the
h’s, each block is one layer of summaries. You can see that it automatically
learned when it’s crossing these bands. The tradeoff is that now you need lots
more layers to get the same processor (it’s not just two anymore). So, it’s more
automatic, but it’s a bit harded to understand. This is sort of the eternal
paradox of deep learning.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Okay, another exercise! Take a few minutes to try this out, work with your
neighbor.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;That’s everything for gated RNNs. Let’s move on to soft attention. This class of
techniques is built of a single observation. We’ve been acting like each new
summary h[t] needs to compress the entire past, when in reality, we may be able
to refer to summaries we computed before. Our summaries can have structure. And
some amount of structure will let us navigate better. Good models do more than
just compress information, they compress information in a way that lets us
easily navigate it.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;We’ll work through this using a canonical application in sequence to sequence
modeling: language translation. The goal is to translate a sentence from one to
another. I want you to think of this geometrically. Each word is represented by
a point in some embedding space, so a sentence is really a sequence of vectors.
The sentences in two languages are two sequences in the embedding space. The
reason translation is possible is because the sequences will have similar
shapes. You basically learn to pattern match different phrases between the
languages – the patterns are small subsequences that can be matched between the
two languages. Corresponding parts of the h space have similar meanings.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;So, from the most abstract point of view, we are learning to transform one
sequence to another which has a similar shape.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Now, to tackle the translation problem, we can make use of a slight variant of
gated RNNs. We take the original sequence, and encode them into a sequence of
summaries h. The last summary h is supposed to capture the entire shape of the
source sentence. We use that to initialize a decoder, which builds a new
sequence of summaries (these blue ones). The point is that, if your meaning is
currently \tilde{h}[t - 1], and your previous word was y[t - 1], then a good
guess for the meaning of the next word is tilde{h}[t]. From the meaning’s
\tilde{h}[t], you should be able to predict a most plausible y[t]. Unlike usual
gated RNNs, you also are allowed to look at this last summary h[T], at any
timepoint you want.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;A word of caution. You never see the real y[t - 1]. If you could, you wouldn’t
need to do the translation task in the first place. So when I said you take
y[t - 1] as input, I lied. You would usually make a prediction of the word y[t]
at each step, based on the current \tilde{h}[t], and use that predicted sequence
as if it were the truth. The distance between the decoded and true sequences is
basically the error of your translation model.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Now, where does attention fit in? The point is that it’s a huge burden on this
last summary, to have to memorize the entire sequence. It’s helped that we use
gating, but it’s not enough. Instead, it’s reasonable to think that, to identify
the meaning for some word near the beginning of the target sentence, it’s enough
to pay attention solely to the summaries near the start of the source sentence.
This is because the meanings are usually similar, between the parts of the
sentence.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Concretely, at the t^th decoding step, we now are allowed to look at some
weighted combination of the source encodings. The weights should be highest on
parts of the source sentence that are most similar in meaning to the part that
we are currently trying to decode.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;When I move along the sentence, I should change the weight distribution.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;This sounds useful, but how do we actually find the weights \alpha? Two
approaches are really common. The first, location based attention, matches
similar points if they occur in similar points in the sentence. The second,
content based attention, looks for similarities in meaning. That’s what I’m
trying to say in this figure. Location based has high weight around the decoding
position. Content based attention puts high weight on these points though (near
the end of the sentence), because, even though they are far in terms of sequence
position, they have similar meanings (remember, location in the embedding space
corresponds to meaning).&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Mathematically, these are computed using these different softmaxes. The first
softmax doesn’t look at the values of the summaries h[t], it doesn’t need to
refer to content. They’re both linear combinations of the current decoding h,
but only the second looks at the earlier encodings.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Okay, another exercise! Take a few minutes to try this out, work with your
neighbor.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;The final big idea of this part of the lecture: hard attention. Sometimes we
have a huge dataset, but only a tiny part is really relevant for the tasks we
have in mind. Is there any way we can avoid doing all this unecessary
computation? For example, let’s say we wanted to figure out what the speed limit
is, from this image. We should learn to attend to this tiny part of the image,
and ignore everything else.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Our approach will be to crop the image into a few relevant pieces, make a good
summary of these pieces, and then use them to classify.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Let’s address each of these components, one at a time. Suppose we knew where to
crop, the idea is to extract a few windows around the crop location, at
different zoom levels. Then, along with the proposed location, we encode
everything into g[t]. The encodings are actually really straightforwards,
they’re just MLPs.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Once we have these encodings, how will we classify? This picture should remind
you a lot of the earlier part of this talk…&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;We can use an RNN. We treat the sequence of encoded crops as a sequence of
inputs to an RNN. The orange rectangles are now a sequence of summaries, just
like the coin flipping summaries from before. The last of these summaries should
have a memory of all the encodings we’ve seen so far. This is what we’ll use for
the final classification.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;There is this lingering issue of where to define the cropping locations. But the
RNN perspective basically solves this problem. We can use the latest summary ot
predict the next crop location. This should make sure that we crop out important
locations that we haven’t looked at before.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;We’re sooo close, but it’s still not enough. The approach works better if you
have some randomness in where you look. It makes some sense – you don’t want
your previous crops to completely predetermine your next proposal. Instead, they
sample the crops around some mean, and the mean is a function of the previous
summary states. Conceptually, this is nice, but it introduces some difficulties
with training. We can’t just backpropagate everything.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;More precisely, we want to propose a mean cropping location in such a way that,
across many random samples around that location, these classification errors are
small. This is different from the usual neural network objective, where we’re
trying to minimize a deterministic loss, which depends on the network
parameters.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Fortunately, there is work on how you do this. I could just show you some math,
but I think it’s better if we look at some plots.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;A refresher, how does gradient descent work? You need to be able to evaluate the
loss at the current parameter \(\mu\). Then, you move your parameter in the
negative gradient direction.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;The only problem with using gradient descent now is that… we see neither
\(f\left(\mu\right)\)… nor it’s gradient. So, we flat out just can’t do
gradient descent. We observe these red points, which are random crops around the
deterministic mean. How should we update our parameter? Well, if you look at
this plot, it makes sense to move it in a direction where the f’s typically seem
pretty small.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;It turns out that this is exactly what this mathematical identity tells you to
do. To take the gradient of this expectation, you can average over a weighted
score function. These terms are directions, relative to the current parameter
value. The weights are higher for points that have higher values of f. If all
the f’s were comparable, then this is basically 0, since the locations have mean&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;But if some of the f’s are much larger than others, we’ll move in those
directions. (well really, we move in the negative gradient direction, since
we’re trying to minimize the function).&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;
&lt;p&gt;Here’s an interactive illustration of that idea. It takes some time, but
eventually it learns to put the mean near the the bottom of this function.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;At this point, you have everything you need to implement a recurrent hard
attention model. You have modules for cropping images and making predictions,
and you know a way to optimize functions that have an element of randomness.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;And that wraps up part 1. There were three major topics: gating, soft attention,
and hard attention. Gating made sure that inputs from early on could still
influence summaries at a later point, making meaningful compression possible
across long sequences. Soft attention let us navigate a larger collection of
local summaries. And hard attention allowed us to focus on just the parts of the
input that matters, reducing a lot of the computational burden.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;In the second part, we’ll discuss more sophisticated approaches modules for
building these summaries. With that, thank you, and take it away Aidan!&lt;/p&gt;
</description>
        <pubDate>Mon, 26 Aug 2019 16:05:00 -0500</pubDate>
        <link>http://localhost:4000/posts/attention-5</link>
        <guid isPermaLink="true">http://localhost:4000/posts/attention-5</guid>
      </item>
    
      <item>
        <title>Teaching attention, part 4 / N: Soft Attention</title>
        <description>&lt;p&gt;There is more to memory than compression. A reduction of information can have
&lt;em&gt;structure&lt;/em&gt; – for example, in a gaussian mixture model, the sufficient
statistics have are the means within each cluster. Depending on which cluster
you might be visiting, you might focus on a different one of the means. A good
compression should induce an effortless way of searching through different parts
of a summary. In essence,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Good models let us navigate information.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In deep learning, this process of navigating information is called “attending
to” different inputs. We’ll illustrate a canonical attention mechanism in the
context of sentence translation, but it’s worth noting that the principle
applies to many settings with structured outputs (like graphs), where the
influences from input to output are somehow localized.&lt;/p&gt;

&lt;p&gt;In the abstract, sentence translation is a prediction problem where the inputs
are sequences and the outputs are also sequences. The input and output are the
same sentence in a source and a target language, respectively. Each element of
the sequence is a word in the sentence. Words can be represented either by
one-hot encodings from some vocabulary (usually, the words in a corpus above
some minimum frequency), or by embeddings derived from some previously trained
model on another corpus. This is illustrated in the figure below.&lt;/p&gt;

&lt;figure&gt;
&lt;img src=&quot;http://localhost:4000/images/soft_attn_figs/sentence_example.svg&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;Once you make the leap into representing words as numerical vectors, it’s
natural to think of sentences as curves in some high-dimensional space. The two
sentences are just two curves in two spaces, and you need to learn to predict
the second from the first. The reason this problem is doable is because, after
seeing enough training examples, you might associated certain subsequences (mini
curve shapes) in the source space with subsequences in the target space.&lt;/p&gt;

&lt;figure&gt;
&lt;img src=&quot;http://localhost:4000/images/soft_attn_figs/sentence_curves.svg&quot; /&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;sequence-to-sequence-architecture&quot;&gt;Sequence-to-Sequence Architecture&lt;/h3&gt;

&lt;p&gt;I’m tempted to call the first approach to this problem the “naive approach,”
except that it doesn’t feel right to call something naive that was
state-of-the-art only a couple of years ago [1].&lt;/p&gt;

&lt;p&gt;The idea is to build a link between the two sequence shapes, using an
appropriate learned representation. You can think of it as a two step process,&lt;/p&gt;

&lt;p&gt;(1) Build a memory of the first sequence, which you’ll be able to refer to later
on.
(2) Learn to associate the memory of the first sequence with a reasonable shape
for the second.&lt;/p&gt;

&lt;p&gt;Step (1) is done with an RNN encoder, just like what we’ve been exploring in the
previous posts. The final state \(h_{T}\) (the final green bar) gives a succint
representation of the entire sequence (the red dots). More than being an
unsupervised representation, it’s one that’s been tailored exactly for the
problem of decoding into the target language.&lt;/p&gt;

&lt;p&gt;The second step is also accomplished by an RNN, but with a a couple twists. As
in usual RNNs, you build up a summary, let’s call it \(\tilde{h}_t\), based on the
previous timepoints summary, and also a new input, the previous target word
\(y_t\). The idea is that the summary is giving a sense of what the sequence means
so far, which is useful for figuring out how it will evolve (certain shapes are
much more common than others). The first twist is that you also are allowed to
look at the summary the source sequence, the original \(h_T\) (practically, this
is just another linear input to the RNN unit).&lt;/p&gt;

&lt;p&gt;The second twist is that the inputs are not properly specified, because they
aren’t actually available at test time. If you knew \(y_t\), you wouldn’t need to
bother with prediction in the first place! Instead, you should build a separate
prediction model from \(\tilde{h}_t\) to \(\hat{y}\) and use the predictions for the
actual inputs (an alternative, called “teacher-forcing,” uses the real \(y_t\)
during training time, can have advantages, but comes with it’s own issues). This
process is illustrated below. The dashed line are the predictions that you have
access to, and the length of the yellow curves gives a sense of the loss.&lt;/p&gt;

&lt;figure&gt;
&lt;img src=&quot;http://localhost:4000/images/soft_attn_figs/decoding_approx.svg&quot; /&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;attention&quot;&gt;Attention&lt;/h3&gt;

&lt;p&gt;The decoding process above puts a lot of burden on the final encoded \(h_T\) from
the first sequence. This seems easily avoidable – it’s not like we need to
throw away all the previous summaries because of computational constraints.
Moreover, you would expect that the summaries \(h_t\) and \(\tilde{h}_t\) might be
closely related in timepoints that are close together. The beginning of the
source and target sentences should mean essentially the same thing, even though
they are in different languages.&lt;/p&gt;

&lt;p&gt;Soft attention is the name of a type of learning module designed directly around
this observation [2]. The idea is to use a different summary at each step of
decoding. Instead of only ever using \(h_T\), we’re allowed to use some
combination \(\sum_{i} \alpha_i h_i\) of the encoder summaries, where the weights
\(\alpha_i\) depend on the timepoint of the decoder. These weights are illustrated
by the pink distributions in the figure below.&lt;/p&gt;

&lt;figure&gt;
&lt;img src=&quot;http://localhost:4000/images/soft_attn_figs/attn_distn_1.svg&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;At the next timepoint, the attetion distribution is free to change, according to
the new position of the decoder. This extra degree of flexibility – the fact
that the RNN decoder has access to different encoder summaries at different
timepoints, not just the unchanging \(h_T\) – is the property that distinguishes
a decoder with an attetion mechanism from a ordinary decoder. A updated attetion
distribution, associated with the next decoder timepoint, is show in the figure
below.&lt;/p&gt;

&lt;figure&gt;
&lt;img src=&quot;http://localhost:4000/images/soft_attn_figs/attn_distn_2.svg&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;Exercise: Write out pseudocode for the forwards pass of a sequece-to-sequece
model with an attetion mechanism in the decoder.&lt;/p&gt;

&lt;p&gt;How does the model know what \(\alpha_i\) distributions are most appropriate for
the different sentences? After all, it must be learned from the data, somehow.
There is no single consensus on how to define these weights, and many papers
have been written specifically around novel proposals for guiding attention
effectively. That said, two approaches are representative of a wide class of
mechanisms [3],&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Local attention: At step \(t\) in decoding, define \(\alpha_i \propto
\exp\left(w_1^{T} \tilde{h}_t, \dots, w_T^{T}\tilde{h}_t\right)\). Or, in pseudocode,
the \(\alpha\) vector can be computed as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a = softmax(l(h_t))&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;l =
nn.linear(dim(h_t), T)&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Content-based attention: At step \(t\) in decoding, define \(\alpha_i \propto
\exp\left(h_1^{T}\tilde{h}_t, \dots, h_T^{T}\tilde{h}_t\right)\). In
pseudocode, this is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a = softmax(H_enc @ h_t)&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;H_enc&lt;/code&gt; is formed by
stacking the summaries \(h_1, \dots, h_T\) as rows.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The main difference is that local attention does not refer to the encoder
summaries \(h_t\). It puts high mass on some parts of the source sentence
depending solely on the state of the decoder \(\tilde{h}_t\). The decoder
summaries tend to reflect, in some of their coordinates, how far along into the
target sentence they are. So, these attention distributions tend to just place
high mass on timepoints in the encoder close to the current timepoint in the
decoder.&lt;/p&gt;

&lt;p&gt;The content-based attention mechanism, on the other hand, allots weight
\(\alpha_i\) according to how similar the source and target encodings are (a large
inner product corresponds to a small angle). This means that if there is are words
in the source sentence that have similar meaning \(h_t\) to the current decoder
summary, \(\tilde{h}_t\), then those words will be upweighted, no matter their
position in the sentence. This is qualitatively illustrated in the figure below.
The content-based approach places high mass near the end of the encoder
sentence, because it is close (in the embedding space), to the meaning at the
decoder timepoint. This is in spite of the fact that those words occur at a
timepoint very different than the current decoder timepoint.&lt;/p&gt;

&lt;h3 id=&quot;discussion&quot;&gt;Discussion&lt;/h3&gt;

&lt;p&gt;No matter how we define the \(\alpha_i\)’s, the fact that we learn them has a nice
consequence: for free, we’ve learned a soft-alignment between the source and
target sentence. At any timepoint in decoding, the encodings with high
\(\alpha_i\) are said to “softly-align” to that timepoint. In this way, you might
notice “inversions,” where the decoder doesn’t just proceed linearly down the
encoder, placing all mass on \(h_t\) when decoding \(\tilde{h}_t\). This is common
in translation between languages that put adjectives before / after nouns, for
example.&lt;/p&gt;

&lt;p&gt;A final observation – including this mechanism is especially beneficial when
working with long sentences. This is phenomenon documented in the literature
[2], and it makes sense in retrospect: long sentences are more suceptible to the
vanishing gradient problem, and even with gating, the final \(h_T\) tends to
mostly reflect cotributions from the end of the source sentence. A approach with
an attetion mechaism, in contrast, can always draw on summaries taken near the
start of the sentence, by putting most of the mass of the \(\alpha_i\) there.&lt;/p&gt;

&lt;p&gt;Finally, while we’ve focused on attention in a sequence to sequence context,
it’s worth noting that these principles extend much more generally. Whenever you
have a structured output or collection of tasks, it’s possible to refer to
summaries selectively, attending to only those that are most relevant for the
current element in the more complex structure / collection. For example, in a
graph problem, only the nearby nodes might be worth attending to. The general
takeaway is that, whenever you might be able to exploit specific structure in
the relationship between summaries $h_t$, you can design an attention mechanism
that makes learning from them simpler.&lt;/p&gt;

&lt;p&gt;Exercise Solution:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// Encode
all_h = []
for t in range(T):
  h = rnn_encoder(x[t], h[t - 1])
  all_h.append(h)

// Decode with attention
h_tilde, y_hat = all_h[-1], []
for t in range(T_prime):
  y_hat.append(pred(h_tilde))
  alpha = attn_weights(y_hat[-1], h_tilde, all_h)
  h_tilde = rnn_decoder(y_hat[-1], h_tilde, alpha * all_h)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;[1] Sutskever, I., Vinyals, O., &amp;amp; Le, Q. V. (2014). Sequence to sequence learning with neural networks. In Advances in neural information processing systems (pp. 3104-3112).&lt;/p&gt;

&lt;p&gt;[2] Bahdanau, D., Cho, K., &amp;amp; Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.&lt;/p&gt;

&lt;p&gt;[3] Luong, M. T., Pham, H., &amp;amp; Manning, C. D. (2015). Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025.&lt;/p&gt;
</description>
        <pubDate>Tue, 20 Aug 2019 09:04:00 -0500</pubDate>
        <link>http://localhost:4000/posts/attention-4</link>
        <guid isPermaLink="true">http://localhost:4000/posts/attention-4</guid>
      </item>
    
      <item>
        <title>Teaching attention, part 3 / N: Thinking more creatively</title>
        <description>&lt;p&gt;To this point, our memory mechanisms all have had the same flavor. We had a
running summary \(h_t\), which we update with every new input, though gating
might mean we prevent some coordinates from changing too much. However, it’s
possible to think more creatively about how we should record and attend to new
memories. For example,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We don’t need to throw away all but the last \(h_t\) when making some
prediction. We’ve been saying that a model “forgets” if there are values for
\(h_t\) early on which are erased by new inputs. But what if we just kept all
the \(h_t\)’s, and avoided overload by having a smart lookup strategy?&lt;/li&gt;
  &lt;li&gt;Why are we forcing the memory \(h_t\) to architecturally mirror the inputs
\(x_t\) – that is, why does \(h_t\) need to be a sequence? Considering that,
due to gating, we only update some cells at a time, it makes sense to design
architectures specifically adapted to recording and looking up useful
summaries.&lt;/li&gt;
  &lt;li&gt;We’ve assumed a pretty diligent sequential processor, which knows to check
every input datapoint as it arrives. What if we had a lazy one, which played
it fast and loose and tried to process only those inputs that seemed really
crucial to the process.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These sorts of preposterous questions will help illuminate the more fundamental
principles of memory and attention in deep learning&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;First though, a warning, reiterating points from part 1. Attention is not a
well-defined, clearly demarcated mathematical term. It is a principle that is
useful for reasoning about many problems, but which is always in flux, as people
find variants that are suited to their particular settings.&lt;/p&gt;

&lt;p&gt;To keep our discussion grounded, and to leave you with something mathematical
examples (and not just cool paper titles) that you can take home, we’re going to
explore a few attention mechanisms in detail, before briefly touring across
other highlights of the memory and attention literature.&lt;/p&gt;

&lt;p&gt;Our three methods mirror the three questions from before.&lt;/p&gt;

&lt;h2 id=&quot;attention-in-translation-models&quot;&gt;Attention in Translation Models&lt;/h2&gt;

&lt;p&gt;We don’t need to force \(h_t\) to condense everything from \(x_1, \dots, x_t\).
This observation lies at the heart of attention-based encoder-decoder sequence
translation models. Before explaining the mechanism in more detail, a refresher
on how translation works with neural networks.&lt;/p&gt;

&lt;p&gt;Abstractly, the problem of translation is to map a sequence of words in one
language to the corresponding sequence in another. For example,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I don’t speak French –&amp;gt; Je ne parle pas le Francais.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We make this an ML problem by treating each word as a datapoint in a sequence,
and hoping to map the first trajectory into the second. That is, we want to map&lt;/p&gt;

\[\begin{align}
\left(x_1, x_2, x_3, x_4\right) \to \left(y_1, y_2, y_3, y_4, y_5, y_6\right)
\end{align}\]

&lt;p&gt;where \(x_i = \left(0, \dots, 0, 1, 0, \dots, 0\right)\) is a one-hot encoding
of the \(i^{th}\) word in the sentence (with respect to the \(V\) words in
English that we bother taking into account), and \(y_i\) does the same for
French.&lt;/p&gt;

&lt;p&gt;The encoder-decoder approach to the problem is to take the \(x_t\) and summarize
(“encode”) them using an RNN&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;. The final state $h_{T}$ is then used as the
used as the initial state to a separate RNN, whose states are used to predict
the words of the translation. Here’s a basic picture. The shading means that teh
same values are copied over.&lt;/p&gt;

&lt;p&gt;At the \(t^{th}\) time step during decoding, we use a processor \(s_t =
f_{\text{dec}}\left(\hat{y}_{t - 1}, h_{t - 1}\right)\), and then predict
\(p\left(y_t \vert s_t, s_{0}, \hat{y}_{t - 1}\right)\) using logistic
regression. A good processor should be able to capture the salient features of
\(x_1, \dots, x_{T}\), and these are used for translation.&lt;/p&gt;

&lt;p&gt;This seems like a large burden for the encoder to carry. Intuitively, at time
\(t\) of decoding, you really only care about the words in the corresponding
neigorhood of time \(t\) in the source sentence. If the sentences were already
aligned, you’d expect the neighborhood around \(h_t\) to be most relevant for
being able to translate \(y_t\).&lt;/p&gt;

&lt;p&gt;So, consider the mechanism, \(s_{t^{\prime}} = \sum_{t = 1}^{T} w_{t t^{\prime}}
h_t\), which adds columns \(h_t\) weighted by \(w_t t^{prime}\)’s, which we call attention
weights. Of course, we need the \(w_{tt^{\prime}}\)’s to be learned directly
from data – this is the same problem as in gatings. The solution shouldn’t be a
surprise: we use a differentiable, parameterized function. For example, we could
use&lt;/p&gt;

\[\begin{align}
w_{t t^{\prime}} \propto a_{W_{a}}\left(h_t, s_{t^{\prime}}\right) = h_t^{T} W_{a} s_{t^{\prime}}
\end{align}\]

&lt;p&gt;to measure the angle between particular coordinates of the encodings.&lt;/p&gt;

&lt;p&gt;It’s worth highlighting what &lt;em&gt;doesn’t&lt;/em&gt; directly appear in this definition – the
times \(t\) and \(t^{\prime}\) themselves. This version of attention is a purely
“content-based” form of attention, which cares only about the encodings \(h_t\)
and \(s_t^{\prime}\). There are variants that take the times themselves into
account, so that the decoding at time \(t^{\prime}\) explicitly upweights
encodings at around that time in the original sequence&lt;sup id=&quot;fnref:2:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;There are a lot of moving parts here – an encoder, a decoder, and an attention
module to link them together. But everything is end-to-end differentiable, and
you can take the gradients of mistranslations with respect to all parameters.&lt;/p&gt;

&lt;p&gt;Stepping back, this is a way of reweighting – or attending to – specific
encodings as a function of our current position in the decoding task at hand. We
have provided some much needed relief for our lone encoder by more richly
relating elements of our differentiable system. We no longer need a processor
that compresses everything into one statistic \(h_{T}\) because we have a more
sophisticated way of navigating a large collection of summaries and pulling out
what is useful.&lt;/p&gt;

&lt;p&gt;This kind of search for locally relevant information is a hallmark of attention
mechanisms. More specifically, this pattern of breaking a task into pieces
\(t^{\prime}\) and finding a way to learn different distributions \(w_{\cdot
t^{\prime}}\) over relevant summaries is one that appears througout the
literature on soft-attention mechanisms.&lt;/p&gt;

&lt;h2 id=&quot;neural-turing-machines&quot;&gt;Neural Turing Machines&lt;/h2&gt;

&lt;p&gt;This discussion suggests that more carefully structured (if potentially
intricate) mechanisms for looking thigns up can alleviate problems of
remembering information that was introduced long ago. This is actually a general
principle, not just some quirk about encoder-decoder architectures. To
illustrate this, we consider the Neural Turing Machine.&lt;/p&gt;

&lt;p&gt;Again, there is a bit going on. The diagram below introduces the cast of
characters.&lt;/p&gt;

&lt;p&gt;Given new input, we produce a weight distribution for what seem like the most
relevant memory locations. Notice that if the rows correspond to times \(1,
\dots, T\), then the matrix \(M\) is a lot like a stacked version of the \(h_1,
\dots, h_T\) from the RNN models before. And, the \(w_{t}\)’s do seem eerily
like the attention weights from before. That said, we’re trying to free
ourselves from the idea that memories must be indexed by time.&lt;/p&gt;

&lt;p&gt;In this spirit of generalization, we can define soft-attention
variant,&lt;/p&gt;

\[\begin{align}
r_t &amp;amp;= \sum_{i} w_{ti} M\left[i, :\right].
\end{align}\]

&lt;p&gt;Something in this architecture which &lt;em&gt;is&lt;/em&gt; new, and not just a variant of what we
saw before in translation, is a notion of writing to memory. We use weights,
along with the add and erase vectors, to define a memory update,&lt;/p&gt;

&lt;p&gt;\(\begin{align}
M\left[i: \right] \xleftarrow M\left[i, :\right] + w_{ti}\left(a_t - e_t \circ M\left[i, :\right]\right).
\end{align}\)
If the weight is small, we don’t change that location in memory at all. If it is
large, we add in \(a_t\) and erase everything that had been at that row, with an
aggressiveness controlled by \(e_t\) (coordinates where that vector is near
\(1\) are almost entirely reset).&lt;/p&gt;

&lt;p&gt;The only remaining issue is how to figure out a good combination of parameters
\(w_t, a_t\), and \(e_t\), upon seeing a new input \(x_t\).&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;I mean, if there are any. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;See, for example, “local attention” in http://arxiv.org/abs/1508.04025. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:2:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Fri, 19 Jul 2019 09:12:00 -0500</pubDate>
        <link>http://localhost:4000/posts/attention-3</link>
        <guid isPermaLink="true">http://localhost:4000/posts/attention-3</guid>
      </item>
    
      <item>
        <title>Teaching attention, part 2 / N: In which I don't even talk about attention</title>
        <description>&lt;p&gt;It’s a little cliche to start a post by saying that a certain type of data (in
this case, sequential data) are everywhere. There’s a risk of being vague, and
writing things that will become outdated, like the phrase, “&lt;a href=&quot;https://www.cia.gov/library/readingroom/docs/CIA-RDP75-00149R000100400004-6.pdf&quot;&gt;soviet
cybernetics&lt;/a&gt;”.&lt;/p&gt;

&lt;p&gt;But sequential data &lt;em&gt;are&lt;/em&gt; everywhere, and they are here to stay. The words in
this sentence are sequential data. The order of bus stops you took to get here
are sequential data. They are everywhere in society – the current capacity of
water reservoirs, the cost of rice, the reach of an influenza outbreak. They are
also everywhere in science – the trajectory of high energy particles at the
LHC, and the sequence of biological reactions that allow cells to divide. The
history of the top trending animal videos on youtube are sequential data&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Even data that are not inherently sequential are often reformulated to be so:
large images or databases can be processed one piece at a time.&lt;/p&gt;

&lt;p&gt;These data are complicated because everything is related to everything, even if
only weakly. A lot of hte videos trending from last week are still popular.&lt;/p&gt;

&lt;p&gt;This interconnectedness makes things harder to manage, because you can’t just
discard a datapoint once you’ve seen it, which is more or less what we do in
settings where we claim the data are i.i.d&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;. A the same time, trying to keep
track of everything is untenable – you would become overloaded very quickly.&lt;/p&gt;

&lt;p&gt;What we need is a way to summarize and store everything that is important, as it
arrives. We also need a way of revisiting relevant information on an as-needed
basis. In the deep learning literature, these two processes are implemented
through mechanisms for memory and attention.&lt;/p&gt;

&lt;h2 id=&quot;rnns-revisited&quot;&gt;RNNs, Revisited&lt;/h2&gt;

&lt;p&gt;Before answering such lofty questions, we will need to have a foundation in the
basics&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;. We begin with the humblest of sequential data proceses, the
recurrent neural network (RNN) cell,&lt;/p&gt;

\[\begin{align}
h_t &amp;amp;= f_{\theta}\left(x_t, h_{t - 1}\right).
\end{align}\]

&lt;p&gt;\(x_t\) are your raw input data at the \(t^{th}\) position in your sequence (“time
\(t\)”). Think of \(h_{t}\) as a running summary of everything you’ve seen so far.
Concretely, the input and summary data are just vectors: \(x_t \in
\mathbb{R}^{n}, h_{t} \in \mathbb{R}^{k}\). The vectors are related by the
function \(f_{\theta}\), parameterized by \(\theta\), which updates the current
summary based on new data \(x_t\).&lt;/p&gt;

&lt;p&gt;For example, let’s say that in our summary, we only care about the number of
times our time series enters the interval \(\left[0, 1\right]\). Our series
should look like this:&lt;/p&gt;

&lt;figure&gt;
&lt;a href=&quot;https://observablehq.com/@krisrs1128/untitled/3&quot;&gt;
  &lt;img src=&quot;http://localhost:4000/images/attn-2-rnn-series.png&quot; /&gt;
&lt;/a&gt;
  &lt;figcaption&gt;
  Click on the figure to go to an interactive version. The original input data
  are the upper series, and the summary is given in the lower panel, counting
  the number of distinct blue segments seen so far.
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This can be easily implemented according by choosing a function \(f_{\theta}\)
appropriately&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;,&lt;/p&gt;

\[\begin{align}
h_{t + 1} = f_{\theta}\left(x_{t + 1}, h_t\right) 
:= \begin{pmatrix}
\mathbb{1}\{x_{t + 1} \in \left[0, 1\right]\} \\
h_{t,2} + \mathbb{1}\{h_{t, 1} = 0 \text{ and } x_{t + 1} \in \left[0, 1\right]\}
\end{pmatrix}
\end{align}.\]

&lt;p&gt;The two coordinates of \(h_t\) encode two features. The first checks whether
\(x_t\) is in $\left[0, 1\right]$. The second coordinate checks whether we’re in
this range, and if and if at the previous step we &lt;em&gt;weren’t&lt;/em&gt;, then it increments
its value, because an entrance has just occurred.&lt;/p&gt;

&lt;figure&gt;
&lt;a href=&quot;https://observablehq.com/@krisrs1128/rnn-entrance-counting&quot;&gt;
  &lt;img src=&quot;http://localhost:4000/images/attn-2-rnn-states.png&quot; /&gt;
&lt;/a&gt;
  &lt;figcaption&gt;
  Again, you can click on the figure to go to an interactive version. The two
  rows of rectangles correspond to the two coordinates of the hidden state. The
  first coordinate activates whenever the series passes through the blue range,
  and the second uses this to build a counter.
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This is maybe one of the simplest examples of a system with memory. It takes a
complicated stream of input, and carefully records the number of \(\left[0,
1\right]\) entrances. If you task at the end of each sequence were to report
this count, then you can call it a day, this processor $f_{\theta}$ solves that
problem entirely.&lt;/p&gt;

&lt;h2 id=&quot;okay-its-not-that-actually-that-simple&quot;&gt;Okay it’s not that actually that simple&lt;/h2&gt;

&lt;p&gt;The general definition \(f_{\theta}\) I’ve given is a technically correct one,
but in our formulation of a counter, we’ve committed a grave sin of deep
learning: we have hard-coded our features. We looked at the problem, declared
that the number of \(\left[0, 1\right]\) entrances was what really mattered, and
hand-engineered the necessary processing unit.&lt;/p&gt;

&lt;p&gt;In the real-world, this usually won’t be possible. Consider the problem of
sentiment analysis – you’re trying to tell whether a movie review is positive
or negative. No one’s going to debate that the number of occurrences of
“fantastic” and “&lt;a href=&quot;https://www.imdb.com/list/ls076596691/&quot;&gt;this movie is honestly the worst movie I’ve ever
seen&lt;/a&gt;” are probably very good
predictors, but it would be foolish to try to engineer all possibly predictive
features by hand.&lt;/p&gt;

&lt;h2 id=&quot;enter-deep-learning&quot;&gt;Enter, Deep Learning&lt;/h2&gt;

&lt;p&gt;To resolve this, we appeal to one of the central tenets of deep learning,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;By composing simple differentiable units, we can learn useful features
automatically.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Graphically, we shift our perspective from chains of RNN cells,&lt;/p&gt;

&lt;p&gt;where \(f_{\theta}\) might be a complex hand-engineered function, to chains &lt;em&gt;and
layers&lt;/em&gt; of simpler processing units.&lt;/p&gt;

&lt;p&gt;The left-to-right arrows are still used to build a type of temporal memory, but
now we have bottom-up function composition arrows, which help in learning
complex features from otherwise simple units. A common “atom” from which these
sheets are defined is the function,&lt;/p&gt;

\[\begin{align}
h_{t} &amp;amp;= \tanh\left(W_{x}x_{t} + W_{h}h_{t - 1} + b\right)
\end{align}\]

&lt;p&gt;which has the form \(f_{\theta}\left(x_t, h_t - 1\right)\) where \(\theta =
\{W_{x}, W_{h}, b\}\). It’s admittedly a relatively restricted class of
functions. Through \(W_{x}\) and \(W_{h}\), it can tell whether certain linear
mixtures of components of \(x_{t}\) or \(h_{t}\) are large or small, but in the
end it can only return monotone functions of this mixtures, and restricted to
\(\left[-1, 1\right]\) at that. That makes things sound more abstract than they
really are – you can actually visualize the entire family of functions in the
case that the input and summary are both two-dimensional,&lt;/p&gt;

&lt;p&gt;These are simple, but by stacking them, we can achieve complexity, just like how
stacks of ReLU units can become complicated feature detectors in computer
vision. In fact, we can almost recover our \(\left[0, 1\right]\) entrance
counter just by stacking a few of these units.&lt;/p&gt;

&lt;p&gt;Define the first layer by,&lt;/p&gt;

\[\begin{align}
h^{1}_{t} &amp;amp;= \tanh\left(\begin{pmatrix} + \\ - \end{pmatrix}x + \begin{pmatrix} 0 \\ -\end{pmatrix}\right)
\end{align}\]

&lt;p&gt;so that the first coordinate tells us if we’re larger than 0, and the second
tells us if we’re less than 1. Here, \(+\) and \(-\) are arbitrary positive and
negative values, respectively.&lt;/p&gt;

&lt;p&gt;In the second layer, we combine these two pieces of information, to see if we’re
in the \(\left[0, 1\right]\) range,&lt;/p&gt;

\[\begin{align}
h_{t}^{2} &amp;amp;= \tanh(1^{T} h_{t}^{1}).
\end{align}\]

&lt;p&gt;These two units are visualized in the interactive figure linked below.&lt;/p&gt;

&lt;figure&gt;
&lt;a href=&quot;https://observablehq.com/@krisrs1128/composition-and-counting&quot;&gt;
  &lt;img src=&quot;http://localhost:4000/images/counter_layers.png&quot; /&gt;
&lt;/a&gt;
  &lt;figcaption&gt;
  When composed, simple, smooth monotone functions can start approximating
  things like indicators (and much more).
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In the last, we need to introduce a sense of history,&lt;/p&gt;

\[\begin{align}
h_{t}^{3} &amp;amp;= \tanh\left(w_{1} h_{t - 1}^{3} + w_{2} h_{t}^{2}\right)
\end{align}\]

&lt;p&gt;Remember, we want to build a counter, but only have access to \(\tanh\)-like
units, which are bounded&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;. The here is to use the first term, \(w_{1}{h_{t -
1}^{3}}\) to maintain a memory of the current counter state, while the second,
\(w_{2}{h_{t}^{2}}\) adds to the counter whenever it detects that we have made
a \(\left[0, 1\right]\) entrance.&lt;/p&gt;

&lt;p&gt;The figure below shows that this more or less works, but that there is a
tradeoff. Making \(w_{1} \gg w_{2}\) means that increments to the count are hard
to notice, but they are retained in memory for a long time. On the other hand,
when \(w_1 &amp;lt;&amp;lt; w_2\), we can easily see when we’ve entered the state, but there
is a risk that the counter “forgets” what it’s earlier value was.&lt;/p&gt;

&lt;h2 id=&quot;gating-and-memory&quot;&gt;Gating and Memory&lt;/h2&gt;

&lt;p&gt;What we’re noticing is an instance of the “vanishing gradient” problem, which is
elegantly illustrated by Figure 4.1&lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; from Graves’ Sequence Labeling book,&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;http://localhost:4000/images/graves_vg_411.png&quot; /&gt;
  &lt;figcaption&gt;
  By nature of the RNN update mechanism, new inputs gradually overwrite new
  ones, and it becomes hard for the network to remember anything from long ago.
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;There is a remarkably simple solution to this problem: gating. The idea is that
if the value of a feature at a particular point is potentially important for
something later on, we block off any updates to it that would otherwise be made
by all the incoming data. This process of blocking off updates is called
“gating”. It’s like we decide the current value is important, and we place it
under a glass case, where it is untouched by anything else in the world. At any
point, it’s value can be read, but not changed.&lt;/p&gt;

&lt;p&gt;If at some future point, we decide we need to change the feature’s value, we
take it out of the case. We ungate what had been a gated value.&lt;/p&gt;

&lt;p&gt;How do you actually implement something like this? In principle, you could
define a binary value for each feature coordinate, demarcating whether the value
can be changed at the current timestep. Then, you could try to learn a good
pattern of 0s and 1s from training examples – you might realize that certain
types of input are useful in the long run, and should always be gated.&lt;/p&gt;

&lt;p&gt;In practice, learning binary patterns is difficult, since such a mask wouldn’t
be differentiable. Instead, we use sigmoid units, which are smooth surrogates
that achieve the same effect. If you pursue this line of thinking for long
enough, you woudl probably arrive at something similar (if not identical) to
LSTM or GRUs&lt;sup id=&quot;fnref:7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h3 id=&quot;mechanics-of-grus&quot;&gt;Mechanics of GRUs&lt;/h3&gt;

&lt;p&gt;A GRU is defined by the update,&lt;/p&gt;

\[\begin{align}
h_t &amp;amp;= \left(1 - z_t\right)h_{t - 1} + z_{t}\tilde{h}_{t}
\end{align}\]

&lt;p&gt;which interpolates between the previous state and some new candidate, according
to some factor \(z_{t}\). To make \(z_{t}\) learnable, it’s set to a sigmoid
over inputs and states,&lt;/p&gt;

\[\begin{align}
z_t &amp;amp;= \sigma\left(W_z x_t + U_z h_{t - 1}\right)
\end{align}\]

&lt;p&gt;The candidate is tricker, because it has a notion of a reset \(r_{t}\),&lt;/p&gt;

\[\begin{align}
\tilde{h}_{t} &amp;amp;= \tanh\left(Wx_t + U\left(r_t \circ h_{t - 1}\right)\right)
\end{align}\]

&lt;p&gt;I think of \(z_t\) as a kind of hesitant forgetting: overwriting you have to do
in order to write anything new to \(h_t\), while \(r_t\) is a brutal
“deliberate” forgetting, which wasn’t strictly necessary.&lt;/p&gt;

&lt;p&gt;Note that to adapt the toy counter, it’s only necessary to introduce gatings
through \(z_t\)’s.&lt;/p&gt;

&lt;h2 id=&quot;finally-a-real-example&quot;&gt;Finally, a real example&lt;/h2&gt;

&lt;p&gt;To be honest, I didn’t want to spend so much time on gated RNNs. However,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;So many of the diagrams explaining these ideas are so complicated (at least
to me, I mean, as useful as they are, does any get geometric intuition from
computational graphs?), that I couldn’t just copy and paste&lt;sup id=&quot;fnref:8&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
  &lt;li&gt;Gating is central to understanding memory and attention. All the other
mechanisms discussed in this series are variants of the basic recipe of
knowing what to write as features for long-term reference and having clear
ways to access them when needed.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I’m also a little tired of coming up with all these weights by hand – don’t
take your optimizers for granted!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Exercise: Train a gated RNN to perform the \(\left[0, 1\right]\) entrance
counting task, starting from a random initialization. How does the number of
layers / number of units in each layer affect the ease of training? How does
that relate to what we know about being able to hand-craft an RNN that solves
the task using only very few units.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To see this working on a real-world task, I’ve trained a gated RNN on the
language modeling problem from this practical. Everything is the same as before,
but the \(x_t\) are now words, which are one-hot vectors in \(\mathbb{R}^{V}\),
and the features \(h_t\) are passed into linear classifiers to predict the word
\(x_{t + 1}\).&lt;/p&gt;

&lt;p&gt;Rows of the \(h_t\) that are constant over regions are examples of memory in
action.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;For that matter, the videos themselves are sequences of image frames. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://stats.stackexchange.com/questions/17391/what-are-i-i-d-random-variables&quot;&gt;Independent and identically distributed&lt;/a&gt;, sometimes called “the big lie of machine learning.” &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;It turns out that the state-of-the-art are just clever combinations of these core components, anyways. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Notice that this definition doesn’t have any parameters, the notation \(\theta\) is actually superfluous here. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;In theory, we could use a linear layer to get integer counts. But this would violate the principle of creating generic architectures, which discover features with minimal hand-crafted input. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Some of the most compelling graphics are the least intricate! &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;These are cool-kids shorthand for the otherwise dissonant phrases “Long Term Short Term Memory” and “Gated Recurrent Units.” Don’t worry about the names, we’ll explain the math in a second, at least for GRUs. &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Believe me, if I could, I would. &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Mon, 15 Jul 2019 13:43:00 -0500</pubDate>
        <link>http://localhost:4000/posts/attention-2</link>
        <guid isPermaLink="true">http://localhost:4000/posts/attention-2</guid>
      </item>
    
      <item>
        <title>Teaching attention, part 1 / N</title>
        <description>&lt;p&gt;Why is it hard to make sense of attention in the deep learning literature?&lt;/p&gt;

&lt;p&gt;I’ve been preparing some notes for a tutorial on memory and attention, and while
the material for memory seems to be coming together relatively smoothly,
designing a coherent discussion around attention has been harder.&lt;/p&gt;

&lt;p&gt;The main difficulty seems to be that the same word (“attention”) is used to
refer to many distinct things in practice. For example, there are differences
between&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Implicit vs. explicit attention&lt;/li&gt;
  &lt;li&gt;Location-based vs. content-based attention&lt;/li&gt;
  &lt;li&gt;Hard vs. soft attention&lt;/li&gt;
  &lt;li&gt;External vs. self attention&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It’s not worth going into the distinctions now – the point is that, you have to
be able to hold many different definitions in your head, in order to hold a
conversation on attention. In the same spirit, if you talk to someone who says
that their model “uses attention,” you have to be a bit of a gadfly to figure
out what’s actually going on.&lt;/p&gt;

&lt;p&gt;Learning many definitions in and of itself wouldn’t be such a big problem,
though it does take a concerted effort. But I suspect that the issue runs a bit
deeper, and that attention is treated more like a problem solving technique,
rather than any concrete computational device. The analogy that comes to mind is
the standard machine in analysis: you use it to prove all sorts of theorems, and
people become comfortable using it in conversations, but no one ever really
bothers to define it.&lt;/p&gt;

&lt;p&gt;This can make it complicated to talk attention across problem domains. While it
speaks the general utility of attention as a problem solving technique, it can
be a little discomforting to have to keep track of many small variants on
something that you thought you understood. Here’s a small sampling of problem
areas where I’ve seen attention come in handy,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Text translation&lt;/li&gt;
  &lt;li&gt;Image classification in presence of clutter&lt;/li&gt;
  &lt;li&gt;Caption generation&lt;/li&gt;
  &lt;li&gt;Speech recognition&lt;/li&gt;
  &lt;li&gt;Image generation&lt;/li&gt;
  &lt;li&gt;Language modeling&lt;/li&gt;
  &lt;li&gt;Algorithm learning (from examples)&lt;/li&gt;
  &lt;li&gt;Handwriting generation&lt;/li&gt;
  &lt;li&gt;Graph modeling&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In each domain, you can usually find any number papers claiming to have solved
the problem using attention. A jaded researcher (&lt;em&gt;cough cough&lt;/em&gt;) would think that
people have discovered that a recipe for writing papers is to pick a random
combination of types of attention and apply it to a problem area (“hard
content-based self-attention for handwriting generation”), but let’s be
positive and say that human creativity knows no limits.&lt;/p&gt;

&lt;p&gt;Within and across these domains, people propose different attention mechanisms.
For example, both DRAW and Self-Attention GANs both have a way of sequentially
attending to parts of an image during generation, but that’s about the only
thing in common. In contrast, it’s not hard to find attention mechanisms for
language translation that differ by things as seemingly inconsequential as
concatenating or linearly mixing inputs (turns out choices like this &lt;em&gt;do&lt;/em&gt; have
an impact on training and performance).&lt;/p&gt;

&lt;p&gt;In some domains, certain heuristics are useful, but they are far from universal.
For example, the “translation” analogy is useful in modalities like image
captioning and speech recognition, not just language. But it doesn’t apply at
all for, say, self-attention. Alternatively, consider the cognitive science
parallel to foveal attention. This is a nice way to understand models that
sequentially process parts of an image, but what to make of attention in
settings that have no clear cognitive science parallel (prediction on graphs
using graph attention?). Another common analogy is to differentiable computers
– attention modules can be thought of as reading from memory on an as-needed
basis. But even this analogy (which I prefer) can lead to to some confusion, as
architectures inspired by this analogy often explicitly include writing
mechanisms to accompany attention, and these can mechanisms can exist
independently.&lt;/p&gt;

&lt;p&gt;One small quirk, just to add to the confusion, is that attention has a
meandering history. Location-based and hard attention appeared first, and while
they are no longer dominant, there are still reasons to prefer them in certain
circumstances, which make them very relevant in current research. Papers
involving attention also have a bad habit of making a splash – I’m sure you can
name a few attention papers by name – but as a consequence, attention might
seem like a hat trick that you can pull to solve a few specialized problems
(e.g., translation and caption generation), rather than problem solving
principle that can be applied in more general settings.&lt;/p&gt;

&lt;h2 id=&quot;but-not-all-is-lost&quot;&gt;But Not All is Lost&lt;/h2&gt;

&lt;p&gt;Just writing about how key ideas are covered in the overgrowth of novel research
doesn’t actually help anyone. It also doesn’t do justice to people who &lt;em&gt;have&lt;/em&gt;
created excellent explanations of attention – I’m thinking of Alex Graves’
lectures and several posts on the distill blog.&lt;/p&gt;

&lt;p&gt;What are strategies that seem useful for learning about attention?&lt;/p&gt;

&lt;p&gt;A starting point seems to be understanding a few examples in depth. I mean,
being able to describe the problem and architectural context, and then saying
what new computational / mathematical operations are introduced which warrant
the name “attention.” Visual, geometric, and probabilistic interpretations –
even those in toy, sandbox examples – can also build fluency. And being able to
implement them in a deep learning library of course doesn’t hurt.&lt;/p&gt;

&lt;p&gt;Second, it seems worth building a catalog of examples, appearing across the
literature. This gives reference points when encountering new ideas, abstract
things become concrete when they can be compared to something you are confident
you understand. In my mind, if I could take two random papers talking about
attention, and if I could relate them to one another (what is similar? what is
different?), only then would I say that I deeply understand attention.&lt;/p&gt;

&lt;p&gt;But this is probably not the right way to think of it. It’s probably more
fruitful to think of shades of understanding in this situation – in some of its
variants, or in some of its contexts, attention might be a familiar friend, in
others, it might require a bit more digging to see what is happening and why
anyone would have wanted it to have happened in that way. This isn’t a bad
thing, it just means you might discover yourself learning afresh something that
you thought you had mastered long ago.&lt;/p&gt;

&lt;p&gt;Anyways, isn’t it impressive how much I’ve written without actually defining
even one example of attention? But this is just part one of N…&lt;/p&gt;
</description>
        <pubDate>Sat, 13 Jul 2019 18:40:00 -0500</pubDate>
        <link>http://localhost:4000/posts/attention-1</link>
        <guid isPermaLink="true">http://localhost:4000/posts/attention-1</guid>
      </item>
    
      <item>
        <title>Nepal AI Winter School Lecture Notes</title>
        <description>&lt;p&gt;Last month, I prepared a series of lectures for the (really wonderfully
organized) first &lt;a href=&quot;https://nepalschool.naamii.com.np/&quot;&gt;Nepal AI Winter School&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The main talks were,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://drive.google.com/open?id=1cpYlXURNsoWnmCfKAG3HGvZh9SeIG2Ps&quot;&gt;Deep Learning Foundations&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://drive.google.com/open?id=1d60GI41oSrldtMKcCOwaVom1TRm0Z8sF&quot;&gt;Humanitarian AI&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://drive.google.com/open?id=13YDzxi8BerO20VzSC2__TEchgwnr-r8L&quot;&gt;Useful Layers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://drive.google.com/open?id=17Rd96ADwoSUFi5M_VT0L8326gnLl-vka&quot;&gt;Bayesian Deep Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://drive.google.com/open?id=1OHNLoAr_om59VSkwMEFQky6aeagx0eQR&quot;&gt;GANs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://drive.google.com/open?id=1b9jVlG2vS2-WU9p6R7-QuRv5uEqHy3Yn&quot;&gt;Metalearning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://drive.google.com/open?id=12aDxwkUYSU5vt3dcg7N3j3NsTahLe6hC&quot;&gt;Interpretability&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The lecture videos are online &lt;a href=&quot;https://www.naamii.com.np/first-nepal-winter-school-in-ai-lecture-videos-and-slides/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I also prepared a short &lt;a href=&quot;https://drive.google.com/open?id=16iQEE_5gD0HYG3CRJWpK-BzxdmSrouUr&quot;&gt;exercise
sheet&lt;/a&gt;
(&lt;a href=&quot;https://drive.google.com/open?id=1g1q5J6BIKCcMemvMok98mjlJ_Bq5ZV3a&quot;&gt;solutions&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;In retrospect, I would have given a more visual introduction to VAEs. As a
placeholder until I write about this in depth, here are some
&lt;a href=&quot;https://drive.google.com/file/d/1Ka0nBX6IpOrIwvZP6ajQDOsLynAGANhf/view?usp=sharing&quot;&gt;notes&lt;/a&gt;
I kept from a discussion with students from a lab session during the school. I
think the most useful pair of pictures was the per-pixel gaussians from the
encoder (first page bottom left) and latent-space axis-aligned gaussians (second
page top right)&lt;/p&gt;
</description>
        <pubDate>Mon, 07 Jan 2019 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/posts/nepal-school-notes</link>
        <guid isPermaLink="true">http://localhost:4000/posts/nepal-school-notes</guid>
      </item>
    
  </channel>
</rss>
