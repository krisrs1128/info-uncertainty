---
title: "Teaching attention, part 2 / N"
date: 2019-07-15 13:43 -4
---

It's a little cliche to start a post by saying that a certain type of data (in
this case, sequential data) are everywhere. There's a risk of being vague, and
writing things that will become outdated, like the phrase, "[soviet
cybernetics](https://www.cia.gov/library/readingroom/docs/CIA-RDP75-00149R000100400004-6.pdf)".

BuThe two coordinates of $$h_t$$ encode two features. The first checks whether
$$x_t$$ is in $\left[0, 1\right]$. The second coordinate checks whether we're in
this range, and if and if at the previous step we _weren't_, then it
increments its value, because an entrance has just occurred.
t sequential data *are* everywhere, and they are here to stay. The words in
this sentence are sequential data. The order of bus stops you took to get here
are sequential data. They are everywhere in society -- the current capacity of
water reservoirs, the cost of rice, the reach of an influenza outbreak. They are
also everywhere in science -- the trajectory of high energy particles at the
LHC, and the sequence of biological reactions that allow cells to divide. The
history of the top trending animal videos on youtube are sequential data[^1].

Even data that are not inherently sequential are often reformulated to be so:
large images or databases can be processed one piece at a time.

These data are complicated because everything is related to everything, even if
only weakly. A lot of hte videos trending from last week are still popular.

This interconnectedness makes things harder to manage, because you can't just
discard a datapoint once you've seen it, which is more or less what we do in
settings where we claim the data are i.i.d[^2]. A the same time, trying to keep
track of everything is untenable -- you would become overloaded very quickly.

What we need is a way to summarize and store everything that is important, as it
arrives. We also need a way of revisiting relevant information on an as-needed
basis. In the deep learning literature, these two processes are implemented
through mechanisms for memory and attention.

## RNNs, Revisited

Before answering such lofty questions, we will need to have a foundation in the
basics[^3]. We begin with the humblest of sequential data proceses, the
recurrent neural network (RNN) cell,

$$
\begin{align}
h_t &= f_{\theta}\left(x_t, h_{t - 1}\right).
\end{align}
$$

$$x_t$$ are your raw input data at the $$t^{th}$$ position in your sequence ("time
$$t$$"). Think of $$h_{t}$$ as a running summary of everything you've seen so far.
Concretely, the input and summary data are just vectors: $$x_t \in
\mathbb{R}^{n}, h_{t} \in \mathbb{R}^{k}$$. The vectors are related by the
function $$f_{\theta}$$, parameterized by $$\theta$$, which updates the current
summary based on new data $$x_t$$.

For example, let's say that in our summary, we only care about the number of
times our time series enters the interval $$\left[0, 1\right]$$. Our series
should look like this.

This can be easily implemented according by choosing a function $$f_{\theta}$$
appropriately[^4],

$$
\begin{align}
h_{t + 1} = f_{\theta}\left(x_{t + 1}, h_t\right) 
:= \begin{pmatrix}
\mathbb{1}\{x_{t + 1} \in \left[0, 1\right]\} \\
h_{t,2} + \mathbb{1}\{h_{t, 1} = 0 \text{ and } x_{t + 1} \in \left[0, 1\right]\}
\end{pmatrix}
\end{align}.
$$

The two coordinates of $$h_t$$ encode two features. The first checks whether
$$x_t$$ is in $\left[0, 1\right]$. The second coordinate checks whether we're in
this range, and if and if at the previous step we _weren't_, then it increments
its value, because an entrance has just occurred.

This works as a counter, you can play around with the figure below. This is
maybe one of the simplest examples of a system with memory. It takes a
complicated stream of input, and carefully records the number of $$\left[0,
1\right]$$ entrances. If you task at the end of each sequence were to report
this count, then you can call it a day, this processor $f_{\theta}$ solves that
problem entirely.

## Everything I have told you is a lie

Alright, not everything. The general definition $$f_{\theta}$$ I've given is a
technically correct one, but in our formulation of a counter, we've committed a
grave sin of deep learning: we have hard-coded our features. We looked at the
problem, declared that the number of $$\left[0, 1\right]$$ entrances was what
really mattered, and hand-engineered the necessary processing unit.

In the real-world, this usually won't be possible. Consider the problem of
sentiment analysis -- you're trying to tell whether a movie review is positive
or negative. No one's going to debate that the number of occurrences of
"fantastic" and "[this movie is honestly the worst movie I've ever
seen](https://www.imdb.com/list/ls076596691/)" are probably very good
predictors, but it would be foolish to try to engineer all possibly predictive
features by hand.

## Enter, Deep Learning

To resolve this, we appeal to one of the central tenets of deep learning,

> By composing simple differentiable units, we can learn useful features
> automatically.

Graphically, we shift our perspective from chains of RNN cells,

where $$f_{\theta}$$ might be a complex hand-engineered function, to chains _and
layers_ of simpler processing units.

The left-to-right arrows are still used to build a type of temporal memory, but
now we have bottom-up function composition arrows, which help in learning
complex features from otherwise simple units. A common "atom" from which these
sheets are defined is the function,

$$
\begin{align}
h_{t} &= \tanh\left(W_{x}x_{t} + W_{h}h_{t - 1} + b\right)
\end{align}
$$

which has the form $$f_{\theta}\left(x_t, h_t - 1\right)$$ where $$\theta =
\{W_{x}, W_{h}, b\}$$. It's admittedly a relatively restricted class of
functions. Through $$W_{x}$$ and $$W_{h}$$, it can tell whether certain linear
mixtures of components of $$x_{t}$$ or $$h_{t}$$ are large or small, but in the
end it can only return monotone functions of this mixtures, and restricted to
$$\left[-1, 1\right]$$ at that. That makes things sound more abstract than they
really are -- you can actually visualize the entire family of functions in the
case that the input and summary are both two-dimensional,

These are simple, but by stacking them, we can achieve complexity, just like how
stacks of ReLU units can become complicated feature detectors in computer
vision. In fact, we can almost recover our $$\left[0, 1\right]$$ entrance
counter just by stacking a few of these units.

[^1]: For that matter, the videos themselves are sequences of image frames.
[^2]: [Independent and identically distributed](https://stats.stackexchange.com/questions/17391/what-are-i-i-d-random-variables), sometimes called "the big lie of machine learning."
[^3]: It turns out that the state-of-the-art are just clever combinations of these core components, anyways.
[^4]: Notice that this definition doesn't have any parameters, the notation $$\theta$$ is actually superfluous here.
